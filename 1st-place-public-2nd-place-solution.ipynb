{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15101533",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94797b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas \"dask[complete]\"\n",
    "!pip install -U pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6531bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:25:59.618292Z",
     "iopub.status.busy": "2022-01-23T02:25:59.616635Z",
     "iopub.status.idle": "2022-01-23T02:26:03.021510Z",
     "shell.execute_reply": "2022-01-23T02:26:03.020936Z",
     "shell.execute_reply.started": "2022-01-19T11:20:17.036084Z"
    },
    "papermill": {
     "duration": 3.439413,
     "end_time": "2022-01-23T02:26:03.021680",
     "exception": false,
     "start_time": "2022-01-23T02:25:59.582267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc  # 가비지 컬렉션 모듈(gc)을 가져옵니다.\n",
    "import glob  # 파일 경로를 찾기 위한 glob 모듈을 가져옵니다.\n",
    "import os  # 운영 체제와 상호 작용하기 위한 os 모듈을 가져옵니다.\n",
    "import time  # 시간 관련 기능을 사용하기 위한 time 모듈을 가져옵니다.\n",
    "import traceback  # 예외 정보를 출력하기 위한 traceback 모듈을 가져옵니다.\n",
    "from contextlib import contextmanager  # 컨텍스트 관리자를 사용하기 위한 contextlib 모듈을 가져옵니다.\n",
    "from enum import Enum  # 열거형을 사용하기 위한 enum 모듈을 가져옵니다.\n",
    "from typing import Dict, List, Optional, Tuple  # 타입 힌트를 사용하기 위한 typing 모듈을 가져옵니다.\n",
    "\n",
    "import seaborn as sns  # 시각화 라이브러리인 seaborn을 가져옵니다.\n",
    "import pandas as pd  # 데이터 조작을 위한 pandas 라이브러리를 가져옵니다.\n",
    "import numpy as np  # 수학 및 배열 연산을 위한 numpy 라이브러리를 가져옵니다.\n",
    "import matplotlib.pyplot as plt  # 시각화 라이브러리인 matplotlib를 가져옵니다.\n",
    "import lightgbm as lgb  # 경량 부스팅 머신러닝 라이브러리인 lightgbm을 가져옵니다.\n",
    "from IPython.display import display  # IPython에서 출력을 보여주기 위한 display 모듈을 가져옵니다.\n",
    "\n",
    "from joblib import delayed, Parallel  # 병렬 처리를 위한 joblib 모듈을 가져옵니다.\n",
    "from sklearn.decomposition import LatentDirichletAllocation  # LDA(Latent Dirichlet Allocation)를 사용하기 위한 모듈을 가져옵니다.\n",
    "from sklearn.manifold import TSNE  # t-SNE(t-Distributed Stochastic Neighbor Embedding)를 사용하기 위한 모듈을 가져옵니다.\n",
    "from sklearn.model_selection import GroupKFold  # 그룹 기반 교차 검증을 위한 모듈을 가져옵니다.\n",
    "from sklearn.neighbors import NearestNeighbors  # 최근접 이웃 알고리즘을 사용하기 위한 모듈을 가져옵니다.\n",
    "from sklearn.preprocessing import minmax_scale  # 데이터 스케일링을 위한 모듈을 가져옵니다.\n",
    "from tqdm import tqdm_notebook as tqdm  # 진행 상황을 표시하기 위한 tqdm 모듈을 가져옵니다.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = ''  # 데이터 디렉토리를 지정합니다.\n",
    "\n",
    "# 데이터 구성 설정\n",
    "USE_PRECOMPUTE_FEATURES = True  # train.csv의 미리 계산된 특징을 로드합니다(속도 향상을 위해 사용)\n",
    "\n",
    "# 모델 및 앙상블 설정\n",
    "PREDICT_CNN = True  # CNN 모델 예측 여부\n",
    "PREDICT_MLP = True  # MLP 모델 예측 여부\n",
    "PREDICT_GBDT = True  # GBDT 모델 예측 여부\n",
    "PREDICT_TABNET = False  # TabNet 모델 예측 여부\n",
    "\n",
    "GBDT_NUM_MODELS = 5  # GBDT 모델 개수\n",
    "GBDT_LR = 0.02  # GBDT 학습률\n",
    "\n",
    "NN_VALID_TH = 0.185  # 최근접 이웃 유효성 임계값\n",
    "NN_MODEL_TOP_N = 3  # 최근접 이웃 모델 상위 N개 선택\n",
    "TAB_MODEL_TOP_N = 3  # TabNet 모델 상위 N개 선택\n",
    "ENSEMBLE_METHOD = 'mean'  # 앙상블 방법('mean' 또는 'other')\n",
    "NN_NUM_MODELS = 10  # 최근접 이웃 모델 개수\n",
    "TABNET_NUM_MODELS = 5  # TabNet 모델 개수\n",
    "\n",
    "# GPU 할당 절약을 위한 설정\n",
    "IS_1ST_STAGE = False  # 1단계인지 여부\n",
    "SHORTCUT_NN_IN_1ST_STAGE = False  # GPU 할당 절약을 위한 조기 중단(최근접 이웃 모델)\n",
    "SHORTCUT_GBDT_IN_1ST_STAGE = False  # GPU 할당 절약을 위한 조기 중단(GBDT 모델)\n",
    "MEMORY_TEST_MODE = False  # 메모리 테스트 모드\n",
    "\n",
    "# 변수 중요도 분석을 위한 설정\n",
    "CV_SPLIT = 'time'  # 교차 검증 분할 방법('time': 시계열 KFold, 'group': 그룹 기반 KFold)\n",
    "USE_PRICE_NN_FEATURES = True  # 가격에 의존하는 최근접 이웃 특징 사용 여부\n",
    "USE_VOL_NN_FEATURES = True  # 거래량에 의존하는 최근접 이웃 특징 사용 여부\n",
    "USE_SIZE_NN_FEATURES = True  # 주문량에 의존하는 최근접 이웃 특징 사용 여부\n",
    "USE_RANDOM_NN_FEATURES = False  # 무작위 인덱스를 사용하여 이웃을 집계하는 최근접 이웃 특징 사용 여부\n",
    "\n",
    "USE_TIME_ID_NN = True  # 시간 ID를 기반으로 한 이웃 사용 여부\n",
    "USE_STOCK_ID_NN = True  # 주식 ID를 기반으로 한 이웃 사용 여부\n",
    "\n",
    "ENABLE_RANK_NORMALIZATION = True  # 순위 정규화 사용 여부\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n",
    "\n",
    "\n",
    "def print_trace(name: str = ''):\n",
    "    print(f'{name or \"익명\"}에서 에러가 발생했습니다.')\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9283ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.105533Z",
     "iopub.status.busy": "2022-01-23T02:26:30.104748Z",
     "iopub.status.idle": "2022-01-23T02:26:30.415719Z",
     "shell.execute_reply": "2022-01-23T02:26:30.416330Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.625381Z"
    },
    "papermill": {
     "duration": 0.341799,
     "end_time": "2022-01-23T02:26:30.416539",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.074740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')  # 'train.csv' 파일을 읽어와서 train 변수에 저장합니다.\n",
    "stock_ids = set(train['stock_id'])  # train 데이터의 'stock_id' 열을 추출하여 중복을 제거한 후 stock_ids 변수에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67a8c8",
   "metadata": {
    "papermill": {
     "duration": 0.030693,
     "end_time": "2022-01-23T02:26:30.479103",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.448410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9bcf32",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.573125Z",
     "iopub.status.busy": "2022-01-23T02:26:30.553308Z",
     "iopub.status.idle": "2022-01-23T02:26:30.575466Z",
     "shell.execute_reply": "2022-01-23T02:26:30.575064Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.920189Z"
    },
    "papermill": {
     "duration": 0.067985,
     "end_time": "2022-01-23T02:26:30.575573",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.507588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum  # 열거형을 사용하기 위한 enum 모듈을 가져옵니다.\n",
    "import pandas as pd  # 데이터 조작을 위한 pandas 라이브러리를 가져옵니다.\n",
    "import numpy as np  # 수학 및 배열 연산을 위한 numpy 라이브러리를 가져옵니다.\n",
    "from typing import Dict, List, Optional, Tuple  # 타입 힌트를 사용하기 위한 typing 모듈을 가져옵니다.\n",
    "from joblib import delayed, Parallel  # 병렬 처리를 위한 joblib 모듈을 가져옵니다.\n",
    "\n",
    "\n",
    "class DataBlock(Enum):  # DataBlock 열거형 클래스를 정의합니다.\n",
    "    TRAIN = 1  # 학습 데이터 블록\n",
    "    TEST = 2  # 테스트 데이터 블록\n",
    "    BOTH = 3  # 학습 및 테스트 데이터 블록\n",
    "\n",
    "\n",
    "def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n",
    "    df = pd.read_parquet('./'+directory+'/stock_id={}'.format(stock_id), engine=\"pyarrow\")  # 주어진 디렉토리에서 stock_id에 해당하는 데이터를 읽어옵니다.\n",
    "    #print(df)  # 읽어온 데이터프레임을 출력합니다.\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n",
    "    if block == DataBlock.TRAIN:\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')  # 학습 데이터를 읽어옵니다.\n",
    "    elif block == DataBlock.TEST:\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')  # 테스트 데이터를 읽어옵니다.\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, DataBlock.TRAIN),\n",
    "            load_data(stock_id, stem, DataBlock.TEST)\n",
    "        ]).reset_index(drop=True)  # 학습 및 테스트 데이터를 결합한 후 인덱스를 재설정합니다.\n",
    "\n",
    "\n",
    "def load_book(stock_id: int, block: DataBlock = DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'book', block)  # 주어진 stock_id와 데이터 블록에 해당하는 book 데이터를 읽어옵니다.\n",
    "\n",
    "\n",
    "def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'trade', block)  # 주어진 stock_id와 데이터 블록에 해당하는 trade 데이터를 읽어옵니다.\n",
    "\n",
    "\n",
    "def calc_wap1(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (\n",
    "            df['bid_size1'] + df['ask_size1'])  # wap1을 계산합니다.\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (\n",
    "            df['bid_size2'] + df['ask_size2'])  # wap2를 계산합니다.\n",
    "    return wap\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def make_book_feature(stock_id, block=DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)  # book 데이터를 가져옵니다.\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)  # wap1을 계산하여 'wap1' 열을 추가합니다.\n",
    "    book['wap2'] = calc_wap2(book)  # wap2를 계산하여 'wap2' 열을 추가합니다.\n",
    "    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return).reset_index(drop=True)  # wap1의 로그 수익률을 계산하여 'log_return1' 열을 추가합니다.\n",
    "    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return).reset_index(drop=True)  # wap2의 로그 수익률을 계산하여 'log_return2' 열을 추가합니다.\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return).reset_index(drop=True)  # ask_price1의 로그 수익률을 계산하여 'log_return_ask1' 열을 추가합니다.\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return).reset_index(drop=True)  # ask_price2의 로그 수익률을 계산하여 'log_return_ask2' 열을 추가합니다.\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return).reset_index(drop=True)  # bid_price1의 로그 수익률을 계산하여 'log_return_bid1' 열을 추가합니다.\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return).reset_index(drop=True)  # bid_price2의 로그 수익률을 계산하여 'log_return_bid2' 열을 추가합니다.\n",
    "\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])  # wap1과 wap2의 차이인 wap_balance를 계산하여 'wap_balance' 열을 추가합니다.\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / (\n",
    "            (book['ask_price1'] + book['bid_price1']) / 2)  # 가격 스프레드를 계산하여 'price_spread' 열을 추가합니다.\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']  # bid_price1과 bid_price2의 차이인 bid_spread를 계산하여 'bid_spread' 열을 추가합니다.\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']  # ask_price1과 ask_price2의 차이인 ask_spread를 계산하여 'ask_spread' 열을 추가합니다.\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (\n",
    "            book['bid_size1'] + book['bid_size2'])  # 총 거래량을 계산하여 'total_volume' 열을 추가합니다.\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (\n",
    "            book['bid_size1'] + book['bid_size2']))  # 거래량 불균형을 계산하여 'volume_imbalance' 열을 추가합니다.\n",
    "\n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread': [np.sum, np.mean, np.std],\n",
    "        'bid_spread': [np.sum, np.mean, np.std],\n",
    "        'ask_spread': [np.sum, np.mean, np.std],\n",
    "        'total_volume': [np.sum, np.mean, np.std],\n",
    "        'volume_imbalance': [np.sum, np.mean, np.std]\n",
    "    }\n",
    "\n",
    "    agg = book.groupby('time_id').agg(features).reset_index(drop=False)  # time_id로 그룹화하여 통계량을 계산합니다.\n",
    "    agg.columns = flatten_name('book', agg.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "    agg['stock_id'] = stock_id  # stock_id를 추가합니다.\n",
    "\n",
    "    for time in [450, 300, 150]:\n",
    "        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)  # 주어진 시간 이상의 데이터를 사용하여 통계량을 계산합니다.\n",
    "        d.columns = flatten_name(f'book_{time}', d.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')  # 계산된 통계량을 기존 데이터프레임에 병합합니다.\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_trade_feature(stock_id, block=DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)  # trade 데이터를 가져옵니다.\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return).reset_index(drop=True)  # price의 로그 수익률을 계산하여 'log_return' 열을 추가합니다.\n",
    "\n",
    "    features = {\n",
    "        'log_return': [realized_volatility],\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'size': [np.sum],\n",
    "        'order_count': [np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()  # time_id로 그룹화하여 통계량을 계산합니다.\n",
    "    agg.columns = flatten_name('trade', agg.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "    agg['stock_id'] = stock_id  # stock_id를 추가합니다.\n",
    "\n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)  # 주어진 시간 이상의 데이터를 사용하여 통계량을 계산합니다.\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')  # 계산된 통계량을 기존 데이터프레임에 병합합니다.\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_book_feature_v2(stock_id, block=DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)  # book 데이터를 가져옵니다.\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        try:\n",
    "            price_list = prices.loc[tid].values.flatten()\n",
    "            price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "            ticks[tid] = price_diff[0]\n",
    "        except Exception:\n",
    "            print_trace(f'tid={tid}')\n",
    "            ticks[tid] = np.nan\n",
    "\n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "def make_features(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "        #df = make_extra_features(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c6b213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.639029Z",
     "iopub.status.busy": "2022-01-23T02:26:30.638303Z",
     "iopub.status.idle": "2022-01-23T02:26:41.422225Z",
     "shell.execute_reply": "2022-01-23T02:26:41.421651Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.961136Z"
    },
    "papermill": {
     "duration": 10.818325,
     "end_time": "2022-01-23T02:26:41.422361",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.604036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load feather]  1.149초\n",
      "is 1st stage\n",
      "[books]  3.702초\n",
      "[trades]  1.130초\n",
      "[extra features]  0.033초\n",
      "[books(v2)]  1.005초\n",
      "(428932, 216)\n",
      "(3, 216)\n"
     ]
    }
   ],
   "source": [
    "if USE_PRECOMPUTE_FEATURES:\n",
    "    with timer('load feather'):\n",
    "        df = pd.read_feather('features_v2.f')  # 저장된 캐시 데이터를 불러옵니다.\n",
    "else:\n",
    "    df = make_features(train, DataBlock.TRAIN)  # 학습 데이터를 기반으로 특징을 생성합니다.\n",
    "    # v2\n",
    "    df = make_features_v2(df, DataBlock.TRAIN)  # 추가 특징을 생성합니다.\n",
    "\n",
    "df.to_feather('features_v2.f')  # 생성된 특징을 캐시 파일로 저장합니다.\n",
    "\n",
    "test = pd.read_csv('test.csv')  # 테스트 데이터를 읽어옵니다.\n",
    "if len(test) == 3:\n",
    "    print('is 1st stage')\n",
    "    IS_1ST_STAGE = True\n",
    "\n",
    "if IS_1ST_STAGE and MEMORY_TEST_MODE:\n",
    "    print('use copy of training data as test data to immitate 2nd stage RAM usage.')\n",
    "    test_df = df.iloc[:170000].copy()\n",
    "    test_df['time_id'] += 32767\n",
    "    test_df['row_id'] = ''\n",
    "else:\n",
    "    test_df = make_features(test, DataBlock.TEST)  # 테스트 데이터를 기반으로 특징을 생성합니다.\n",
    "    test_df = make_features_v2(test_df, DataBlock.TEST)  # 추가 특징을 생성합니다.\n",
    "\n",
    "print(df.shape)  # 생성된 학습 데이터의 크기를 출력합니다.\n",
    "print(test_df.shape)  # 생성된 테스트 데이터의 크기를 출력합니다.\n",
    "df = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)  # 학습 데이터와 테스트 데이터를 결합하고 인덱스를 재설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d00a11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>book.seconds_in_bucket.count</th>\n",
       "      <th>book.wap1.sum</th>\n",
       "      <th>book.wap1.mean</th>\n",
       "      <th>book.wap1.std</th>\n",
       "      <th>book.wap2.sum</th>\n",
       "      <th>book.wap2.mean</th>\n",
       "      <th>book.wap2.std</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_450.order_count.mean</th>\n",
       "      <th>trade_300.log_return.realized_volatility</th>\n",
       "      <th>trade_300.seconds_in_bucket.count</th>\n",
       "      <th>trade_300.size.sum</th>\n",
       "      <th>trade_300.order_count.mean</th>\n",
       "      <th>trade_150.log_return.realized_volatility</th>\n",
       "      <th>trade_150.seconds_in_bucket.count</th>\n",
       "      <th>trade_150.size.sum</th>\n",
       "      <th>trade_150.order_count.mean</th>\n",
       "      <th>tick_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>302.0</td>\n",
       "      <td>303.125061</td>\n",
       "      <td>1.003725</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>303.105539</td>\n",
       "      <td>1.003661</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>...</td>\n",
       "      <td>2.642857</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1587.0</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2069.0</td>\n",
       "      <td>2.433333</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.047768</td>\n",
       "      <td>1.000239</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>200.041171</td>\n",
       "      <td>1.000206</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>...</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>16.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>2.041667</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>188.0</td>\n",
       "      <td>187.913849</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>187.939824</td>\n",
       "      <td>0.999680</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>...</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>120.0</td>\n",
       "      <td>119.859781</td>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>119.835941</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>176.0</td>\n",
       "      <td>175.932865</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>175.934256</td>\n",
       "      <td>0.999626</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>...</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>399.0</td>\n",
       "      <td>399.721741</td>\n",
       "      <td>1.001809</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>399.714325</td>\n",
       "      <td>1.001790</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>...</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5150.0</td>\n",
       "      <td>2.813953</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7261.0</td>\n",
       "      <td>2.822581</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>217.0</td>\n",
       "      <td>217.058914</td>\n",
       "      <td>1.000271</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>217.079727</td>\n",
       "      <td>1.000367</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3010.0</td>\n",
       "      <td>3.588235</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4287.0</td>\n",
       "      <td>3.034483</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428932</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.001215</td>\n",
       "      <td>1.000405</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>3.001650</td>\n",
       "      <td>1.000550</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428933</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428934</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428935 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target  book.seconds_in_bucket.count  \\\n",
       "0              0        5  0.004136                         302.0   \n",
       "1              0       11  0.001445                         200.0   \n",
       "2              0       16  0.002168                         188.0   \n",
       "3              0       31  0.002195                         120.0   \n",
       "4              0       62  0.001747                         176.0   \n",
       "...          ...      ...       ...                           ...   \n",
       "428930       126    32763  0.003357                         399.0   \n",
       "428931       126    32767  0.002090                         217.0   \n",
       "428932         0        4       NaN                           3.0   \n",
       "428933         0       32       NaN                           NaN   \n",
       "428934         0       34       NaN                           NaN   \n",
       "\n",
       "        book.wap1.sum  book.wap1.mean  book.wap1.std  book.wap2.sum  \\\n",
       "0          303.125061        1.003725       0.000693     303.105539   \n",
       "1          200.047768        1.000239       0.000262     200.041171   \n",
       "2          187.913849        0.999542       0.000864     187.939824   \n",
       "3          119.859781        0.998832       0.000757     119.835941   \n",
       "4          175.932865        0.999619       0.000258     175.934256   \n",
       "...               ...             ...            ...            ...   \n",
       "428930     399.721741        1.001809       0.000456     399.714325   \n",
       "428931     217.058914        1.000271       0.000384     217.079727   \n",
       "428932       3.001215        1.000405       0.000170       3.001650   \n",
       "428933            NaN             NaN            NaN            NaN   \n",
       "428934            NaN             NaN            NaN            NaN   \n",
       "\n",
       "        book.wap2.mean  book.wap2.std  ...  trade_450.order_count.mean  \\\n",
       "0             1.003661       0.000781  ...                    2.642857   \n",
       "1             1.000206       0.000272  ...                    2.200000   \n",
       "2             0.999680       0.000862  ...                    3.666667   \n",
       "3             0.998633       0.000656  ...                    3.666667   \n",
       "4             0.999626       0.000317  ...                    3.500000   \n",
       "...                ...            ...  ...                         ...   \n",
       "428930        1.001790       0.000507  ...                    2.727273   \n",
       "428931        1.000367       0.000465  ...                    4.000000   \n",
       "428932        1.000550       0.000153  ...                         NaN   \n",
       "428933             NaN            NaN  ...                         NaN   \n",
       "428934             NaN            NaN  ...                         NaN   \n",
       "\n",
       "        trade_300.log_return.realized_volatility  \\\n",
       "0                                       0.001308   \n",
       "1                                       0.000587   \n",
       "2                                       0.001137   \n",
       "3                                       0.001089   \n",
       "4                                       0.000453   \n",
       "...                                          ...   \n",
       "428930                                  0.001520   \n",
       "428931                                  0.000849   \n",
       "428932                                       NaN   \n",
       "428933                                       NaN   \n",
       "428934                                       NaN   \n",
       "\n",
       "        trade_300.seconds_in_bucket.count  trade_300.size.sum  \\\n",
       "0                                    21.0              1587.0   \n",
       "1                                    16.0               900.0   \n",
       "2                                    12.0              1189.0   \n",
       "3                                     9.0              1556.0   \n",
       "4                                    11.0              1219.0   \n",
       "...                                   ...                 ...   \n",
       "428930                               43.0              5150.0   \n",
       "428931                               17.0              3010.0   \n",
       "428932                                NaN                 NaN   \n",
       "428933                                NaN                 NaN   \n",
       "428934                                NaN                 NaN   \n",
       "\n",
       "        trade_300.order_count.mean  trade_150.log_return.realized_volatility  \\\n",
       "0                         2.571429                                  0.001701   \n",
       "1                         2.250000                                  0.000813   \n",
       "2                         3.166667                                  0.001621   \n",
       "3                         5.111111                                  0.001401   \n",
       "4                         4.909091                                  0.000550   \n",
       "...                            ...                                       ...   \n",
       "428930                    2.813953                                  0.001714   \n",
       "428931                    3.588235                                  0.001012   \n",
       "428932                         NaN                                       NaN   \n",
       "428933                         NaN                                       NaN   \n",
       "428934                         NaN                                       NaN   \n",
       "\n",
       "        trade_150.seconds_in_bucket.count  trade_150.size.sum  \\\n",
       "0                                    30.0              2069.0   \n",
       "1                                    24.0              1173.0   \n",
       "2                                    20.0              2010.0   \n",
       "3                                    11.0              1631.0   \n",
       "4                                    16.0              1570.0   \n",
       "...                                   ...                 ...   \n",
       "428930                               62.0              7261.0   \n",
       "428931                               29.0              4287.0   \n",
       "428932                                NaN                 NaN   \n",
       "428933                                NaN                 NaN   \n",
       "428934                                NaN                 NaN   \n",
       "\n",
       "        trade_150.order_count.mean  tick_size  \n",
       "0                         2.433333   0.000052  \n",
       "1                         2.041667   0.000050  \n",
       "2                         2.950000   0.000048  \n",
       "3                         4.545455   0.000046  \n",
       "4                         4.500000   0.000047  \n",
       "...                            ...        ...  \n",
       "428930                    2.822581   0.000066  \n",
       "428931                    3.034483   0.000051  \n",
       "428932                         NaN   0.000049  \n",
       "428933                         NaN        NaN  \n",
       "428934                         NaN        NaN  \n",
       "\n",
       "[428935 rows x 216 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df#.tail(10)#.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281cfe2",
   "metadata": {
    "papermill": {
     "duration": 0.028421,
     "end_time": "2022-01-23T02:26:41.480303",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.451882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Nearest-Neighbor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aca7436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.555136Z",
     "iopub.status.busy": "2022-01-23T02:26:41.550462Z",
     "iopub.status.idle": "2022-01-23T02:26:41.557458Z",
     "shell.execute_reply": "2022-01-23T02:26:41.557058Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.104849Z"
    },
    "papermill": {
     "duration": 0.048663,
     "end_time": "2022-01-23T02:26:41.557564",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.508901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 80\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: Optional[Dict] = None, \n",
    "                 exclude_self: bool = False):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        \n",
    "        if metric == 'random':\n",
    "            n_queries = len(pivot)\n",
    "            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n",
    "        else:\n",
    "            nn = NearestNeighbors(\n",
    "                n_neighbors=N_NEIGHBORS_MAX, \n",
    "                p=p, \n",
    "                metric=metric, \n",
    "                metric_params=metric_params\n",
    "            )\n",
    "            nn.fit(pivot)\n",
    "            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,:], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.unstack().reset_index()\n",
    "        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n",
    "        return dst\n",
    "# index='time_id', columns='stock_id', values='price'\n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        feature_pivot = df.pivot(index='time_id', columns='stock_id', values=feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n",
    "\n",
    "\n",
    "class StockIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"stock-id based nearest neighbor features\"\"\"\n",
    "        feature_pivot = df.pivot(index='time_id', columns='stock_id', values=feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n",
    "            \n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "\n",
    "\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "972a076e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.619972Z",
     "iopub.status.busy": "2022-01-23T02:26:41.619012Z",
     "iopub.status.idle": "2022-01-23T02:26:41.982083Z",
     "shell.execute_reply": "2022-01-23T02:26:41.981564Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.127333Z"
    },
    "papermill": {
     "duration": 0.395558,
     "end_time": "2022-01-23T02:26:41.982243",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.586685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\n",
    "df['trade.tau'] = np.sqrt(1 / df['trade.seconds_in_bucket.count'])\n",
    "df['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\n",
    "df['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\n",
    "df['real_price'] = 0.01 / df['tick_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a6240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T02:18:50.195022Z",
     "iopub.status.busy": "2022-01-16T02:18:50.1946Z",
     "iopub.status.idle": "2022-01-16T02:18:50.201136Z",
     "shell.execute_reply": "2022-01-16T02:18:50.199965Z",
     "shell.execute_reply.started": "2022-01-16T02:18:50.194964Z"
    },
    "papermill": {
     "duration": 0.030837,
     "end_time": "2022-01-23T02:26:42.050294",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.019457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Build Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64aead97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:42.123778Z",
     "iopub.status.busy": "2022-01-23T02:26:42.122544Z",
     "iopub.status.idle": "2022-01-23T02:33:32.953387Z",
     "shell.execute_reply": "2022-01-23T02:33:32.953798Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.499414Z"
    },
    "papermill": {
     "duration": 410.874751,
     "end_time": "2022-01-23T02:33:32.953989",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.079238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[knn fit]  393.133초\n"
     ]
    }
   ],
   "source": [
    "time_id_neighbors: List[Neighbors] = []\n",
    "stock_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = df[['stock_id', 'time_id']].copy()\n",
    "    df_pv['price'] = 0.01 / df['tick_size'] \n",
    "    df_pv['vol'] = df['book.log_return1.realized_volatility']\n",
    "    df_pv['trade.tau'] = df['trade.tau']\n",
    "    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n",
    "    \n",
    "    if USE_PRICE_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values='price') #  , ''\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_price_c', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='canberra', \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_price_m', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='mahalanobis',\n",
    "                metric_params={'VI':np.cov(pivot.values.T)}\n",
    "            )\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_price_l1', \n",
    "                minmax_scale(pivot.transpose()), \n",
    "                p=1, \n",
    "                exclude_self=True)\n",
    "        )\n",
    "\n",
    "    if USE_VOL_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'vol')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors('time_vol_l1', pivot, p=1)\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_vol_l1', \n",
    "                minmax_scale(pivot.transpose()), \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if USE_SIZE_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'trade.size.sum')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_size_m', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='mahalanobis', \n",
    "                metric_params={'VI':np.cov(pivot.values.T)}\n",
    "            )\n",
    "        )\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_size_c', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='canberra'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    if USE_RANDOM_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'vol')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_random', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='random'\n",
    "            )\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_random', \n",
    "                pivot.transpose(), # Stock ID를 기준열로 바꿔야 하므로 Transpose\n",
    "                p=2,\n",
    "                metric='random')\n",
    "        )\n",
    "\n",
    "\n",
    "if not USE_TIME_ID_NN:\n",
    "    time_id_neighbors = []\n",
    "    \n",
    "if not USE_STOCK_ID_NN:\n",
    "    stock_id_neighbors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27029360",
   "metadata": {
    "papermill": {
     "duration": 0.028479,
     "end_time": "2022-01-23T02:33:33.011086",
     "exception": false,
     "start_time": "2022-01-23T02:33:32.982607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check Neighbor Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fd5a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.073803Z",
     "iopub.status.busy": "2022-01-23T02:33:33.073080Z",
     "iopub.status.idle": "2022-01-23T02:33:33.075920Z",
     "shell.execute_reply": "2022-01-23T02:33:33.075471Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.548287Z"
    },
    "papermill": {
     "duration": 0.035942,
     "end_time": "2022-01-23T02:33:33.076032",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.040090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_rank_correraltion(neighbors, top_n=10):\n",
    "    if not neighbors:\n",
    "        return\n",
    "    neighbor_indices = pd.DataFrame()\n",
    "    for n in neighbors:\n",
    "        neighbor_indices[n.name] = n.neighbors[:,:top_n].flatten()\n",
    "        display(neighbor_indices[n.name])\n",
    "    sns.heatmap(neighbor_indices.corr('kendall'), annot=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5123638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.138908Z",
     "iopub.status.busy": "2022-01-23T02:33:33.138122Z",
     "iopub.status.idle": "2022-01-23T02:33:33.192860Z",
     "shell.execute_reply": "2022-01-23T02:33:33.192422Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.555683Z"
    },
    "papermill": {
     "duration": 0.08837,
     "end_time": "2022-01-23T02:33:33.192975",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.104605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-id NN (name=time_price_c, metric=canberra, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>30183</td>\n",
       "      <td>31471</td>\n",
       "      <td>26708</td>\n",
       "      <td>7864</td>\n",
       "      <td>22752</td>\n",
       "      <td>10619</td>\n",
       "      <td>11453</td>\n",
       "      <td>1205</td>\n",
       "      <td>9352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2811</td>\n",
       "      <td>29583</td>\n",
       "      <td>30798</td>\n",
       "      <td>17639</td>\n",
       "      <td>25131</td>\n",
       "      <td>23202</td>\n",
       "      <td>14857</td>\n",
       "      <td>4739</td>\n",
       "      <td>3399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5829</td>\n",
       "      <td>4275</td>\n",
       "      <td>7783</td>\n",
       "      <td>4487</td>\n",
       "      <td>7845</td>\n",
       "      <td>25439</td>\n",
       "      <td>17530</td>\n",
       "      <td>18634</td>\n",
       "      <td>19747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>6367</td>\n",
       "      <td>19386</td>\n",
       "      <td>1255</td>\n",
       "      <td>12559</td>\n",
       "      <td>19472</td>\n",
       "      <td>18358</td>\n",
       "      <td>31719</td>\n",
       "      <td>6481</td>\n",
       "      <td>26475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>31554</td>\n",
       "      <td>24443</td>\n",
       "      <td>5916</td>\n",
       "      <td>19164</td>\n",
       "      <td>20430</td>\n",
       "      <td>659</td>\n",
       "      <td>31077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  30183  31471  26708   7864  22752  10619  11453   1205    9352\n",
       "11          11   2811  29583  30798  17639  25131  23202  14857   4739    3399\n",
       "16          16   5829   4275   7783   4487   7845  25439  17530  18634   19747\n",
       "31          31   6367  19386   1255  12559  19472  18358  31719   6481   26475\n",
       "32          32     34      4  31554  24443   5916  19164  20430    659   31077"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3833, 10)\n",
      "time-id NN (name=time_price_m, metric=mahalanobis, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>31471</td>\n",
       "      <td>11453</td>\n",
       "      <td>30183</td>\n",
       "      <td>7864</td>\n",
       "      <td>26708</td>\n",
       "      <td>4091</td>\n",
       "      <td>30430</td>\n",
       "      <td>22752</td>\n",
       "      <td>9889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2811</td>\n",
       "      <td>29583</td>\n",
       "      <td>30798</td>\n",
       "      <td>14857</td>\n",
       "      <td>4739</td>\n",
       "      <td>17639</td>\n",
       "      <td>25131</td>\n",
       "      <td>23202</td>\n",
       "      <td>13745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>4275</td>\n",
       "      <td>18634</td>\n",
       "      <td>5829</td>\n",
       "      <td>25439</td>\n",
       "      <td>17530</td>\n",
       "      <td>7783</td>\n",
       "      <td>4034</td>\n",
       "      <td>4487</td>\n",
       "      <td>19747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>12559</td>\n",
       "      <td>17429</td>\n",
       "      <td>26475</td>\n",
       "      <td>31719</td>\n",
       "      <td>18358</td>\n",
       "      <td>6481</td>\n",
       "      <td>7897</td>\n",
       "      <td>12348</td>\n",
       "      <td>9456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>5916</td>\n",
       "      <td>31554</td>\n",
       "      <td>19164</td>\n",
       "      <td>6213</td>\n",
       "      <td>659</td>\n",
       "      <td>25636</td>\n",
       "      <td>24443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  31471  11453  30183   7864  26708   4091  30430  22752    9889\n",
       "11          11   2811  29583  30798  14857   4739  17639  25131  23202   13745\n",
       "16          16   4275  18634   5829  25439  17530   7783   4034   4487   19747\n",
       "31          31  12559  17429  26475  31719  18358   6481   7897  12348    9456\n",
       "32          32     34      4   5916  31554  19164   6213    659  25636   24443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3833, 10)\n",
      "time-id NN (name=time_vol_l1, metric=minkowski, p=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9352</td>\n",
       "      <td>15276</td>\n",
       "      <td>13791</td>\n",
       "      <td>1205</td>\n",
       "      <td>12923</td>\n",
       "      <td>26708</td>\n",
       "      <td>2331</td>\n",
       "      <td>2136</td>\n",
       "      <td>10672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23202</td>\n",
       "      <td>30798</td>\n",
       "      <td>17639</td>\n",
       "      <td>7460</td>\n",
       "      <td>29583</td>\n",
       "      <td>11227</td>\n",
       "      <td>2811</td>\n",
       "      <td>25131</td>\n",
       "      <td>32597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9060</td>\n",
       "      <td>25179</td>\n",
       "      <td>25439</td>\n",
       "      <td>21777</td>\n",
       "      <td>15727</td>\n",
       "      <td>17530</td>\n",
       "      <td>6476</td>\n",
       "      <td>211</td>\n",
       "      <td>30791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>10291</td>\n",
       "      <td>15689</td>\n",
       "      <td>18848</td>\n",
       "      <td>22824</td>\n",
       "      <td>14449</td>\n",
       "      <td>1142</td>\n",
       "      <td>6367</td>\n",
       "      <td>21148</td>\n",
       "      <td>25731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>25584</td>\n",
       "      <td>26883</td>\n",
       "      <td>5235</td>\n",
       "      <td>2772</td>\n",
       "      <td>26430</td>\n",
       "      <td>2502</td>\n",
       "      <td>22014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5   9352  15276  13791   1205  12923  26708   2331   2136   10672\n",
       "11          11  23202  30798  17639   7460  29583  11227   2811  25131   32597\n",
       "16          16   9060  25179  25439  21777  15727  17530   6476    211   30791\n",
       "31          31  10291  15689  18848  22824  14449   1142   6367  21148   25731\n",
       "32          34     32      4  25584  26883   5235   2772  26430   2502   22014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3833, 10)\n",
      "time-id NN (name=time_size_m, metric=mahalanobis, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>30183</td>\n",
       "      <td>23490</td>\n",
       "      <td>22752</td>\n",
       "      <td>26708</td>\n",
       "      <td>20928</td>\n",
       "      <td>13791</td>\n",
       "      <td>1350</td>\n",
       "      <td>31883</td>\n",
       "      <td>10619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>29583</td>\n",
       "      <td>19417</td>\n",
       "      <td>9822</td>\n",
       "      <td>23656</td>\n",
       "      <td>4367</td>\n",
       "      <td>22828</td>\n",
       "      <td>30798</td>\n",
       "      <td>11682</td>\n",
       "      <td>10745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>25439</td>\n",
       "      <td>6121</td>\n",
       "      <td>8168</td>\n",
       "      <td>31443</td>\n",
       "      <td>7845</td>\n",
       "      <td>14721</td>\n",
       "      <td>1040</td>\n",
       "      <td>20630</td>\n",
       "      <td>11497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>13594</td>\n",
       "      <td>16802</td>\n",
       "      <td>20099</td>\n",
       "      <td>31719</td>\n",
       "      <td>1239</td>\n",
       "      <td>19472</td>\n",
       "      <td>3846</td>\n",
       "      <td>12559</td>\n",
       "      <td>13989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>22014</td>\n",
       "      <td>6482</td>\n",
       "      <td>27822</td>\n",
       "      <td>1392</td>\n",
       "      <td>9215</td>\n",
       "      <td>24921</td>\n",
       "      <td>30803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  30183  23490  22752  26708  20928  13791   1350  31883   10619\n",
       "11          11  29583  19417   9822  23656   4367  22828  30798  11682   10745\n",
       "16          16  25439   6121   8168  31443   7845  14721   1040  20630   11497\n",
       "31          31  13594  16802  20099  31719   1239  19472   3846  12559   13989\n",
       "32          32     34      4  22014   6482  27822   1392   9215  24921   30803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3833, 10)\n",
      "time-id NN (name=time_size_c, metric=canberra, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>26708</td>\n",
       "      <td>30183</td>\n",
       "      <td>22752</td>\n",
       "      <td>1205</td>\n",
       "      <td>10619</td>\n",
       "      <td>9352</td>\n",
       "      <td>15276</td>\n",
       "      <td>30620</td>\n",
       "      <td>2683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2811</td>\n",
       "      <td>17639</td>\n",
       "      <td>29583</td>\n",
       "      <td>25131</td>\n",
       "      <td>28020</td>\n",
       "      <td>17604</td>\n",
       "      <td>9822</td>\n",
       "      <td>4739</td>\n",
       "      <td>30798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5829</td>\n",
       "      <td>4487</td>\n",
       "      <td>6121</td>\n",
       "      <td>7783</td>\n",
       "      <td>1040</td>\n",
       "      <td>29026</td>\n",
       "      <td>7845</td>\n",
       "      <td>17530</td>\n",
       "      <td>16118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>6367</td>\n",
       "      <td>12559</td>\n",
       "      <td>22519</td>\n",
       "      <td>18358</td>\n",
       "      <td>7897</td>\n",
       "      <td>19472</td>\n",
       "      <td>31522</td>\n",
       "      <td>19386</td>\n",
       "      <td>31719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>15989</td>\n",
       "      <td>11985</td>\n",
       "      <td>3732</td>\n",
       "      <td>26430</td>\n",
       "      <td>3607</td>\n",
       "      <td>10523</td>\n",
       "      <td>4487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  26708  30183  22752   1205  10619   9352  15276  30620    2683\n",
       "11          11   2811  17639  29583  25131  28020  17604   9822   4739   30798\n",
       "16          16   5829   4487   6121   7783   1040  29026   7845  17530   16118\n",
       "31          31   6367  12559  22519  18358   7897  19472  31522  19386   31719\n",
       "32          32     34      4  15989  11985   3732  26430   3607  10523    4487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3833, 10)\n"
     ]
    }
   ],
   "source": [
    "time_ids = np.array(sorted(df['time_id'].unique()))\n",
    "for neighbor in time_id_neighbors:\n",
    "    print(neighbor)\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            time_ids[neighbor.neighbors[:,:10]], \n",
    "            index=pd.Index(time_ids, name='time_id'), \n",
    "            columns=[f'top_{i+1}' for i in range(10)]\n",
    "        ).iloc[1:6]\n",
    "    )\n",
    "    print(pd.DataFrame(\n",
    "            time_ids[neighbor.neighbors[:,:10]], \n",
    "            index=pd.Index(time_ids, name='time_id'), \n",
    "            columns=[f'top_{i+1}' for i in range(10)]\n",
    "        ).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2589b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.263643Z",
     "iopub.status.busy": "2022-01-23T02:33:33.262935Z",
     "iopub.status.idle": "2022-01-23T02:33:33.276604Z",
     "shell.execute_reply": "2022-01-23T02:33:33.276173Z",
     "shell.execute_reply.started": "2022-01-19T11:39:40.610534Z"
    },
    "papermill": {
     "duration": 0.051151,
     "end_time": "2022-01-23T02:33:33.276704",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.225553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock-id NN (name=stock_price_l1, metric=minkowski, p=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>125</td>\n",
       "      <td>98</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>75</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>113</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>95</td>\n",
       "      <td>78</td>\n",
       "      <td>102</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "      <td>114</td>\n",
       "      <td>72</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>122</td>\n",
       "      <td>116</td>\n",
       "      <td>61</td>\n",
       "      <td>124</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>105</td>\n",
       "      <td>88</td>\n",
       "      <td>72</td>\n",
       "      <td>114</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>116</td>\n",
       "      <td>33</td>\n",
       "      <td>120</td>\n",
       "      <td>97</td>\n",
       "      <td>112</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>87</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>67</td>\n",
       "      <td>23</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "      <td>37</td>\n",
       "      <td>107</td>\n",
       "      <td>43</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>56</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>122</td>\n",
       "      <td>115</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>125</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>30</td>\n",
       "      <td>114</td>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>110</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>95</td>\n",
       "      <td>84</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  \\\n",
       "stock_id                                                                  \n",
       "0             0     93    125     98     51     59     75     30     19   \n",
       "1             1     15     11    113     26     23     95     78    102   \n",
       "2             2     30      4     60     88      5    125    114     72   \n",
       "3             3     97     56     63    122    116     61    124     73   \n",
       "4             4      5     30     50    105     88     72    114      7   \n",
       "...         ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "122         122    116     33    120     97    112     22     29     87   \n",
       "123         123     67     23    102    103     37    107     43     52   \n",
       "124         124     56     36      3    101    122    115     28     29   \n",
       "125         125     93     98      4      2     88     30    114     51   \n",
       "126         126    110     62     11     27     95     84     26     18   \n",
       "\n",
       "          top_10  \n",
       "stock_id          \n",
       "0             17  \n",
       "1             37  \n",
       "2             66  \n",
       "3             29  \n",
       "4              2  \n",
       "...          ...  \n",
       "122          102  \n",
       "123            1  \n",
       "124           97  \n",
       "125           17  \n",
       "126          113  \n",
       "\n",
       "[112 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock-id NN (name=stock_vol_l1, metric=minkowski, p=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>67</td>\n",
       "      <td>59</td>\n",
       "      <td>120</td>\n",
       "      <td>66</td>\n",
       "      <td>16</td>\n",
       "      <td>107</td>\n",
       "      <td>70</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>126</td>\n",
       "      <td>122</td>\n",
       "      <td>112</td>\n",
       "      <td>94</td>\n",
       "      <td>44</td>\n",
       "      <td>101</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>125</td>\n",
       "      <td>13</td>\n",
       "      <td>105</td>\n",
       "      <td>15</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>97</td>\n",
       "      <td>56</td>\n",
       "      <td>61</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>113</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>47</td>\n",
       "      <td>15</td>\n",
       "      <td>125</td>\n",
       "      <td>28</td>\n",
       "      <td>115</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>95</td>\n",
       "      <td>84</td>\n",
       "      <td>102</td>\n",
       "      <td>59</td>\n",
       "      <td>112</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>95</td>\n",
       "      <td>32</td>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>114</td>\n",
       "      <td>108</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>56</td>\n",
       "      <td>96</td>\n",
       "      <td>73</td>\n",
       "      <td>111</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>22</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>125</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>105</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>70</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "      <td>44</td>\n",
       "      <td>95</td>\n",
       "      <td>23</td>\n",
       "      <td>122</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  \\\n",
       "stock_id                                                                  \n",
       "0             0    104     67     59    120     66     16    107     70   \n",
       "1             1     10     95    126    122    112     94     44    101   \n",
       "2             2     68    125     13    105     15     47     14     30   \n",
       "3             3      6     50     97     56     61     55      9    113   \n",
       "4             4    105     47     15    125     28    115     72     19   \n",
       "...         ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "122         122     95     84    102     59    112     52     10     78   \n",
       "123         123     95     32     46     41      0    119    114    108   \n",
       "124         124     56     96     73    111     36     69     22     63   \n",
       "125         125     47     14     13      2    105     20     19     70   \n",
       "126         126      1     10     87     44     95     23    122     36   \n",
       "\n",
       "          top_10  \n",
       "stock_id          \n",
       "0             51  \n",
       "1             84  \n",
       "2             34  \n",
       "3             62  \n",
       "4             20  \n",
       "...          ...  \n",
       "122           90  \n",
       "123           10  \n",
       "124           44  \n",
       "125           93  \n",
       "126           48  \n",
       "\n",
       "[112 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_ids = np.array(sorted(df['stock_id'].unique()))\n",
    "for neighbor in stock_id_neighbors:\n",
    "    print(neighbor)\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            stock_ids[neighbor.neighbors[:,:10]], \n",
    "            index=pd.Index(stock_ids, name='stock_id'), \n",
    "            columns=[f'top_{i+1}' for i in range(10)]\n",
    "        )#.loc[64]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10b5471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.348457Z",
     "iopub.status.busy": "2022-01-23T02:33:33.347681Z",
     "iopub.status.idle": "2022-01-23T02:33:33.901994Z",
     "shell.execute_reply": "2022-01-23T02:33:33.902453Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.542166Z"
    },
    "papermill": {
     "duration": 0.591893,
     "end_time": "2022-01-23T02:33:33.902600",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.310707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           5\n",
       "2           6\n",
       "3        3709\n",
       "4        2924\n",
       "         ... \n",
       "38325    3488\n",
       "38326    1716\n",
       "38327     357\n",
       "38328    1409\n",
       "38329    3434\n",
       "Name: time_price_c, Length: 38330, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           5\n",
       "2           6\n",
       "3         729\n",
       "4        3709\n",
       "         ... \n",
       "38325    1716\n",
       "38326     239\n",
       "38327     801\n",
       "38328    2279\n",
       "38329    3537\n",
       "Name: time_price_m, Length: 38330, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           5\n",
       "2           6\n",
       "3        3051\n",
       "4        3205\n",
       "         ... \n",
       "38325    3624\n",
       "38326     262\n",
       "38327     109\n",
       "38328      72\n",
       "38329    3725\n",
       "Name: time_vol_l1, Length: 38330, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           5\n",
       "2           6\n",
       "3        1966\n",
       "4         793\n",
       "         ... \n",
       "38325    3488\n",
       "38326    3424\n",
       "38327    3050\n",
       "38328    3082\n",
       "38329    2976\n",
       "Name: time_size_m, Length: 38330, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           5\n",
       "2           6\n",
       "3        1936\n",
       "4        1426\n",
       "         ... \n",
       "38325     357\n",
       "38326    1716\n",
       "38327    1272\n",
       "38328    3492\n",
       "38329     247\n",
       "Name: time_size_c, Length: 38330, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_rank_correraltion(time_id_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35df3e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.038369Z",
     "iopub.status.busy": "2022-01-23T02:33:34.037754Z",
     "iopub.status.idle": "2022-01-23T02:33:34.275994Z",
     "shell.execute_reply": "2022-01-23T02:33:34.276373Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.949648Z"
    },
    "papermill": {
     "duration": 0.313195,
     "end_time": "2022-01-23T02:33:34.276516",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.963321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1        81\n",
       "2       110\n",
       "3        86\n",
       "4        46\n",
       "       ... \n",
       "1115     83\n",
       "1116     74\n",
       "1117     23\n",
       "1118     17\n",
       "1119    100\n",
       "Name: stock_price_l1, Length: 1120, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1        92\n",
       "2        59\n",
       "3        52\n",
       "4       106\n",
       "       ... \n",
       "1115     83\n",
       "1116     22\n",
       "1117    107\n",
       "1118     33\n",
       "1119     44\n",
       "Name: stock_vol_l1, Length: 1120, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_rank_correraltion(stock_id_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11498c",
   "metadata": {
    "papermill": {
     "duration": 0.035477,
     "end_time": "2022-01-23T02:33:34.347529",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.312052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Aggregate Features With Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec957b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.427036Z",
     "iopub.status.busy": "2022-01-23T02:33:34.426197Z",
     "iopub.status.idle": "2022-01-23T02:33:36.057149Z",
     "shell.execute_reply": "2022-01-23T02:33:36.056625Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.189634Z"
    },
    "papermill": {
     "duration": 1.674163,
     "end_time": "2022-01-23T02:33:36.057283",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.383120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features with large changes over time are converted to relative ranks within time-id\n",
    "if ENABLE_RANK_NORMALIZATION:\n",
    "    df['trade.order_count.mean'] = df.groupby('time_id')['trade.order_count.mean'].rank()\n",
    "    df['book.total_volume.sum']  = df.groupby('time_id')['book.total_volume.sum'].rank()\n",
    "    df['book.total_volume.mean'] = df.groupby('time_id')['book.total_volume.mean'].rank()\n",
    "    df['book.total_volume.std']  = df.groupby('time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "    df['trade.tau'] = df.groupby('time_id')['trade.tau'].rank()\n",
    "\n",
    "    for dt in [150, 300, 450]:\n",
    "        df[f'trade_{dt}.order_count.mean'] = df.groupby('time_id')[f'trade_{dt}.order_count.mean'].rank()\n",
    "        df[f'book_{dt}.total_volume.sum']  = df.groupby('time_id')[f'book_{dt}.total_volume.sum'].rank()\n",
    "        df[f'book_{dt}.total_volume.mean'] = df.groupby('time_id')[f'book_{dt}.total_volume.mean'].rank()\n",
    "        df[f'book_{dt}.total_volume.std']  = df.groupby('time_id')[f'book_{dt}.total_volume.std'].rank()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c16dbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3833, 80)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_id_neighbors[0].neighbors.shape\n",
    "\n",
    "#test_nn.rearrange_feature_values(df2, feature_col)\n",
    "            \n",
    "#dst = nn.make_nn_feature(10, np.mean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6efc7d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:36.151666Z",
     "iopub.status.busy": "2022-01-23T02:33:36.150004Z",
     "iopub.status.idle": "2022-01-23T02:33:36.152354Z",
     "shell.execute_reply": "2022-01-23T02:33:36.152748Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.199422Z"
    },
    "papermill": {
     "duration": 0.059468,
     "end_time": "2022-01-23T02:33:36.152899",
     "exception": false,
     "start_time": "2022-01-23T02:33:36.093431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    print(df2.shape)\n",
    "\n",
    "    feature_cols_stock = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade_150.tau': [np.mean],\n",
    "        'book.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "    }\n",
    "    \n",
    "    feature_cols = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'real_price': [np.max, np.mean, np.min],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "        'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n",
    "        'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "    stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "    \n",
    "    # 새로운 feature를 기존 df에 추가하는 함수\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            return ndf\n",
    "\n",
    "    # neighbor stock_id\n",
    "    for feature_col in feature_cols_stock.keys():\n",
    "    # 'book.log_return1.realized_volatility', 'trade.seconds_in_bucket.count', 'trade.tau', 'trade_150.tau', 'book.tau', 'trade.size.sum', 'book.seconds_in_bucket.count' \n",
    "        try:\n",
    "            # 해당 특성이 기존 df에 있다면 Pass\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "            # stock_id_neighbors가 아무것도 없다면 Pass\n",
    "            if not stock_id_neighbors:\n",
    "                continue\n",
    "            \n",
    "            # stock_id_neighbors의 각 Class (2개)를 반복\n",
    "            for nn in stock_id_neighbors:\n",
    "                # stock_id_neighbors에 feature_col로 rearrange_feature_values\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "            \n",
    "            # agg : feature_cols_stock의 function pointer (np.mean, np.max ...)\n",
    "            for agg in feature_cols_stock[feature_col]:\n",
    "                for n in stock_id_neighbor_sizes: # n : [10, 20, 40]\n",
    "                    try:\n",
    "                        for nn in stock_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg) \n",
    "                            # e.g. n : 10, agg : np.mean\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                            # add columns\n",
    "                    except Exception:\n",
    "                        print_trace('stock-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('stock-id nn')\n",
    "            pass\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "    ndf = None\n",
    "\n",
    "    print(df2.shape)\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        try:\n",
    "            if not USE_PRICE_NN_FEATURES and feature_col == 'real_price':\n",
    "                continue\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "\n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "            if 'volatility' in feature_col:\n",
    "                time_id_ns = time_id_neigbor_sizes_vol\n",
    "            else:\n",
    "                time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "    \n",
    "    print(df2.shape)\n",
    "    \n",
    "    # features further derived from nearest neighbor features\n",
    "    try:\n",
    "        if USE_PRICE_NN_FEATURES:\n",
    "            for sz in time_id_neigbor_sizes:\n",
    "                denominator = f\"real_price_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
    "                df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
    "\n",
    "            for sz in time_id_neigbor_sizes_vol:\n",
    "                denominator = f\"book.log_return1.realized_volatility_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'vol_rankmin_{sz}'] = \\\n",
    "                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'vol_rankmax_{sz}'] = \\\n",
    "                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amax\"]\n",
    "\n",
    "        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "        for c in price_cols:\n",
    "            del df2[c]\n",
    "\n",
    "        if USE_PRICE_NN_FEATURES:\n",
    "            for sz in time_id_neigbor_sizes_vol:\n",
    "                tgt = f'book.log_return1.realized_volatility_nn{sz}_time_price_m_mean'\n",
    "                df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
    "    except Exception:\n",
    "        print_trace('nn features')\n",
    "    \n",
    "    print(df2.shape)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d78a6c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:36.370271Z",
     "iopub.status.busy": "2022-01-23T02:33:36.368938Z",
     "iopub.status.idle": "2022-01-23T02:34:51.959576Z",
     "shell.execute_reply": "2022-01-23T02:34:51.959095Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.224929Z"
    },
    "papermill": {
     "duration": 75.77123,
     "end_time": "2022-01-23T02:34:51.959705",
     "exception": false,
     "start_time": "2022-01-23T02:33:36.188475",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428935, 220)\n",
      "(428935, 280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428935, 625)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:120: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:121: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_9888/3909245258.py:138: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428935, 582)\n",
      "[make nearest neighbor feature]  70.080초\n",
      "(428935, 582)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df2 = make_nearest_neighbor_feature(df)\n",
    "\n",
    "print(df2.shape)\n",
    "#df2.reset_index(drop=True).to_feather('optiver_df2.f')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68410cfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60d7f2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3833, 112)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_id_neighbors[0].feature_values[0].shape # index ,columns ,feature_values ,  feature_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e3ff0",
   "metadata": {
    "papermill": {
     "duration": 0.037563,
     "end_time": "2022-01-23T02:34:52.038355",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.000792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Misc Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672b203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.076612Z",
     "iopub.status.busy": "2022-01-23T02:34:53.075859Z",
     "iopub.status.idle": "2022-01-23T02:34:53.335135Z",
     "shell.execute_reply": "2022-01-23T02:34:53.334610Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.290787Z"
    },
    "papermill": {
     "duration": 1.258742,
     "end_time": "2022-01-23T02:34:53.335258",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.076516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# skew correction for NN\n",
    "cols_to_log = [\n",
    "    'trade.size.sum',\n",
    "    'trade_150.size.sum',\n",
    "    'trade_300.size.sum',\n",
    "    'trade_450.size.sum',\n",
    "    'volume_imbalance'\n",
    "]\n",
    "for c in df2.columns:\n",
    "    for check in cols_to_log:\n",
    "        try:\n",
    "            if check in c:\n",
    "                df2[c] = np.log(df2[c]+1)\n",
    "                break\n",
    "        except Exception:\n",
    "            print_trace('log1p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b80e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.415634Z",
     "iopub.status.busy": "2022-01-23T02:34:53.414883Z",
     "iopub.status.idle": "2022-01-23T02:34:54.757020Z",
     "shell.execute_reply": "2022-01-23T02:34:54.756480Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.724354Z"
    },
    "papermill": {
     "duration": 1.384579,
     "end_time": "2022-01-23T02:34:54.757155",
     "exception": false,
     "start_time": "2022-01-23T02:34:53.372576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rolling average of RV for similar trading volume\n",
    "try:\n",
    "    df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    roll_target = 'book.log_return1.realized_volatility'\n",
    "\n",
    "    for window_size in [3, 10]:\n",
    "        df2[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n",
    "            df2.groupby('stock_id')[roll_target].rolling(window_size, center=True, min_periods=1) \\\n",
    "                                                .mean() \\\n",
    "                                                .reset_index() \\\n",
    "                                                .sort_values(by=['level_1'])[roll_target].values\n",
    "except Exception:\n",
    "    print_trace('mean RV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac17ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:54.865738Z",
     "iopub.status.busy": "2022-01-23T02:34:54.859008Z",
     "iopub.status.idle": "2022-01-23T02:34:57.630440Z",
     "shell.execute_reply": "2022-01-23T02:34:57.631709Z",
     "shell.execute_reply.started": "2022-01-15T04:54:08.318718Z"
    },
    "papermill": {
     "duration": 2.836215,
     "end_time": "2022-01-23T02:34:57.631962",
     "exception": false,
     "start_time": "2022-01-23T02:34:54.795747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stock-id embedding (helps little)\n",
    "try:\n",
    "    lda_n = 3\n",
    "    lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n",
    "\n",
    "    stock_id_emb = pd.DataFrame(\n",
    "        lda.fit_transform(pivot.transpose()), \n",
    "        index=df_pv.pivot(index='time_id', columns='stock_id', values= 'vol').columns\n",
    "    )\n",
    "\n",
    "    for i in range(lda_n):\n",
    "        df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n",
    "except Exception:\n",
    "    print_trace('LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b1249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:57.804033Z",
     "iopub.status.busy": "2022-01-23T02:34:57.802818Z",
     "iopub.status.idle": "2022-01-23T02:34:59.440689Z",
     "shell.execute_reply": "2022-01-23T02:34:59.441129Z",
     "shell.execute_reply.started": "2022-01-15T04:54:13.038956Z"
    },
    "papermill": {
     "duration": 1.736319,
     "end_time": "2022-01-23T02:34:59.441286",
     "exception": false,
     "start_time": "2022-01-23T02:34:57.704967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df2[~df2.target.isnull()].copy()\n",
    "df_test = df2[df2.target.isnull()].copy()\n",
    "del df2, df_pv\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898c5d1",
   "metadata": {
    "papermill": {
     "duration": 0.037142,
     "end_time": "2022-01-23T02:34:59.516263",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.479121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reverse Engineering time-id Order & Make CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306bfda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.616772Z",
     "iopub.status.busy": "2022-01-23T02:34:59.615063Z",
     "iopub.status.idle": "2022-01-23T02:34:59.617414Z",
     "shell.execute_reply": "2022-01-23T02:34:59.617868Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.7715Z"
    },
    "papermill": {
     "duration": 0.063706,
     "end_time": "2022-01-23T02:34:59.618003",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.554297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "    \n",
    "\n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'], engine='pyarrow')\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        file_paths = glob.glob('book_train.parquet/**/*.parquet', recursive=True)\n",
    "\n",
    "        # stock_id 추출 및 열 생성\n",
    "        df_files = pd.DataFrame({'book_path': file_paths})\n",
    "        df_files['stock_id'] = df_files['book_path'].str.extractall(r'stock_id=(\\d+)').astype(int).reset_index(level=1, drop=True)\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot(index='time_id', columns='stock_id', values= 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    plt.plot(df_ordered['stock_id=61'])\n",
    "    \n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ad6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.705424Z",
     "iopub.status.busy": "2022-01-23T02:34:59.704651Z",
     "iopub.status.idle": "2022-01-23T02:35:01.635920Z",
     "shell.execute_reply": "2022-01-23T02:35:01.636972Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.801275Z"
    },
    "papermill": {
     "duration": 1.981013,
     "end_time": "2022-01-23T02:35:01.637181",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.656168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CV_SPLIT == 'time':\n",
    "    with timer('calculate order of time-id'):\n",
    "        if USE_PRECOMPUTE_FEATURES:\n",
    "            timeid_order = pd.read_csv('time_id_order.csv')\n",
    "        else:\n",
    "            timeid_order = reconstruct_time_id_order()\n",
    "            \n",
    "\n",
    "    with timer('make folds'):\n",
    "        timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
    "        df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
    "        df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "        folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n",
    "        time_id_orders = df_train['time_id_order']\n",
    "\n",
    "        folds = []\n",
    "        for i, border in enumerate(folds_border):\n",
    "            idx_train = np.where(time_id_orders < border)[0]\n",
    "            idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n",
    "            folds.append((idx_train, idx_valid))\n",
    "\n",
    "            print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n",
    "\n",
    "    del df_train['time_id_order']\n",
    "elif CV_SPLIT == 'group':\n",
    "    gkf = GroupKFold(n_splits=4)\n",
    "    folds = []\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(gkf.split(df_train, None, groups=df_train['time_id'])):\n",
    "        folds.append((idx_train, idx_valid))\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020eecf0",
   "metadata": {
    "papermill": {
     "duration": 0.067715,
     "end_time": "2022-01-23T02:35:01.777174",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.709459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6f703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:01.921148Z",
     "iopub.status.busy": "2022-01-23T02:35:01.920367Z",
     "iopub.status.idle": "2022-01-23T02:35:01.933964Z",
     "shell.execute_reply": "2022-01-23T02:35:01.934928Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.902446Z"
    },
    "papermill": {
     "duration": 0.091675,
     "end_time": "2022-01-23T02:35:01.935102",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.843427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "\n",
    "# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # order by average importance across folds\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # plot top-n\n",
    "    PLOT_TOP_N = 50\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n",
    "    return df_src[cols]\n",
    "\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "        features = list(self.models[0].feature_name())\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            assert features == list(m.feature_name())\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "        for i, m in enumerate(self.models):\n",
    "            w = self.weights[i] if self.weights is not None else 1\n",
    "            predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "        return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "    def feature_name(self) -> List[str]:\n",
    "        return self.models[0].feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb222a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:02.048565Z",
     "iopub.status.busy": "2022-01-23T02:35:02.044618Z",
     "iopub.status.idle": "2022-01-23T03:35:55.320275Z",
     "shell.execute_reply": "2022-01-23T03:35:55.319782Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.922483Z"
    },
    "papermill": {
     "duration": 3653.32245,
     "end_time": "2022-01-23T03:35:55.320410",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.997960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = GBDT_LR\n",
    "if SHORTCUT_GBDT_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "    # to save GPU quota\n",
    "    lr = 0.3\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0,\n",
    "    'metric': '',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 5,\n",
    "    'min_data_in_leaf': 1000,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 128,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'learning_rate': lr\n",
    "}\n",
    "\n",
    "X = get_X(df_train)\n",
    "y = df_train['target']\n",
    "\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "X.to_feather('X.f')\n",
    "\n",
    "\n",
    "y = df_train[['target']].reset_index(drop=True)\n",
    "y.to_feather('y.f')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.shape)\n",
    "\"\"\"\n",
    "if PREDICT_GBDT:\n",
    "    ds = lgb.Dataset(X, y, weight=1/np.power(y, 2))\n",
    "\n",
    "    with timer('lgb.cv'):\n",
    "        ret = lgb.cv(params, ds, num_boost_round=8000, folds=folds,\n",
    "             feval=feval_RMSPE, stratified=False,\n",
    "             return_cvbooster=True, verbose_eval=20,\n",
    "             early_stopping_rounds=200, seed=42)\n",
    "# early_stopping_rounds=int(40*0.1/lr))\n",
    "        print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n",
    "\n",
    "    best_iteration = len(ret['RMSPE-mean'])\n",
    "    for i in range(len(folds)):\n",
    "        y_pred = ret['cvbooster'].boosters[i].predict(X.iloc[folds[i][1]], num_iteration=best_iteration)\n",
    "        y_true = y.iloc[folds[i][1]]\n",
    "        print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n",
    "        \n",
    "        if i == len(folds) - 1:\n",
    "            np.save('pred_gbdt.npy', y_pred)\n",
    "\n",
    "    plot_importance(ret['cvbooster'], figsize=(10, 20))\n",
    "\n",
    "    boosters = []\n",
    "    with timer('retraining'):\n",
    "        for i in range(GBDT_NUM_MODELS):\n",
    "            params['seed'] = i\n",
    "            boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n",
    "\n",
    "    booster = EnsembleModel(boosters)\n",
    "    del ret\n",
    "    del ds\n",
    "\n",
    "gc.collect()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b346e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb015e20",
   "metadata": {
    "papermill": {
     "duration": 0.057064,
     "end_time": "2022-01-23T03:35:55.434906",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.377842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce13e5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:55.569557Z",
     "iopub.status.busy": "2022-01-23T03:35:55.558558Z",
     "iopub.status.idle": "2022-01-23T03:35:56.706846Z",
     "shell.execute_reply": "2022-01-23T03:35:56.705908Z",
     "shell.execute_reply.started": "2022-01-15T04:57:16.2193Z"
    },
    "papermill": {
     "duration": 1.211778,
     "end_time": "2022-01-23T03:35:56.706992",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.495214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "\n",
    "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine',\n",
    "                 n_a: int = 16):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = [0]\n",
    "        cat_dims = [128]\n",
    "\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=n_a,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10\n",
    "            #device_name=device,\n",
    "            #clip_value=1.5\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        rmspe = rmspe_metric(y_va, predicted)\n",
    "        best_losses.append(rmspe)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 1e10\n",
    "        best_prediction = None\n",
    "\n",
    "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            model = MLP(X_tr.shape[1],\n",
    "                        n_categories=[128],\n",
    "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
    "        elif model_type == 'cnn':\n",
    "            model = CNN(X_tr.shape[1],\n",
    "                        hidden_size=cnn_hidden,\n",
    "                        n_categories=[128],\n",
    "                        emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb,\n",
    "                        channel_1=cnn_channel1,\n",
    "                        channel_2=cnn_channel2,\n",
    "                        channel_3=cnn_channel3,\n",
    "                        two_stage=False,\n",
    "                        kernel1=cnn_kernel1,\n",
    "                        celu=cnn_celu,\n",
    "                        dropout_top=cnn_dropout,\n",
    "                        dropout_mid=cnn_dropout,\n",
    "                        dropout_bottom=cnn_dropout,\n",
    "                        weight_norm=cnn_weight_norm,\n",
    "                        leaky_relu=cnn_leaky_relu)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = model.to(device)\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='min',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=4)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(rmspe)\n",
    "\n",
    "            if rmspe < best_loss:\n",
    "                print(f'new best:{rmspe}')\n",
    "                best_loss = rmspe\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/dreamquark-ai/tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12245cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:57.029334Z",
     "iopub.status.busy": "2022-01-23T03:35:56.898214Z",
     "iopub.status.idle": "2022-01-23T06:56:27.693883Z",
     "shell.execute_reply": "2022-01-23T06:56:27.695021Z",
     "shell.execute_reply.started": "2022-01-15T04:57:17.584692Z"
    },
    "papermill": {
     "duration": 12030.930352,
     "end_time": "2022-01-23T06:56:27.695345",
     "exception": false,
     "start_time": "2022-01-23T03:35:56.764993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "del df, df_train\n",
    "gc.collect()\n",
    "\n",
    "def get_top_n_models(models, scores, top_n):\n",
    "    if len(models) <= top_n:\n",
    "        print('number of models are less than top_n. all models will be used')\n",
    "        return models\n",
    "    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n",
    "    return [x for _, x in sorted_][:top_n]\n",
    "\n",
    "\n",
    "if PREDICT_MLP:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "    \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 30\n",
    "        valid_th = NN_VALID_TH\n",
    "    \n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        # MLP\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               batch_size=512,\n",
    "                                               mlp_bn=True,\n",
    "                                               mlp_hidden=256,\n",
    "                                               mlp_dropout=0.0,\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs,\n",
    "                                               lr=0.002,\n",
    "                                               max_lr=0.0055,\n",
    "                                               weight_decay=1e-7,\n",
    "                                               model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i)\n",
    "        if nn_losses[0] < NN_VALID_TH:\n",
    "            print(f'model of seed {i} added.')\n",
    "            scores.append(nn_losses[0])\n",
    "            model_paths.append(f'artifacts/mlp_fold_0_seed{i}.pth')\n",
    "            np.save(f'pred_mlp_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    mlp_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(mlp_model)} models will be used.')\n",
    "if PREDICT_CNN:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 50\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               cnn_hidden=8*128,\n",
    "                                               batch_size=1280,\n",
    "                                               model_type='cnn',\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs, #epochs,\n",
    "                                               cnn_channel1=128,\n",
    "                                               cnn_channel2=3*128,\n",
    "                                               cnn_channel3=3*128,\n",
    "                                               lr=0.00038, #0.0011,\n",
    "                                               max_lr=0.0013,\n",
    "                                               weight_decay=6.5e-6,\n",
    "                                               optimizer_type='adam',\n",
    "                                               scheduler_type='reduce',\n",
    "                                               model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i,\n",
    "                                               cnn_dropout=0.0,\n",
    "                                               cnn_weight_norm=True,\n",
    "                                               cnn_leaky_relu=False,\n",
    "                                               patience=8,\n",
    "                                               factor=0.3)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            model_paths.append(f'artifacts/cnn_fold_0_seed{i}.pth')\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'pred_cnn_seed{i}.npy', nn_preds[0])\n",
    "            \n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    cnn_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(cnn_model)} models will be used.')\n",
    "    \n",
    "if PREDICT_TABNET:\n",
    "    tab_model = []\n",
    "    scores = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 10\n",
    "        valid_th = 1000\n",
    "    else:\n",
    "        print('train full')\n",
    "        epochs = 250\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(TABNET_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "                                                          [folds[-1]], \n",
    "                                                          batch_size=1280,\n",
    "                                                          epochs=epochs, #epochs,\n",
    "                                                          lr=0.04,\n",
    "                                                          patience=50,\n",
    "                                                          factor=0.5,\n",
    "                                                          gamma=1.6,\n",
    "                                                          lambda_sparse=3.55e-6,\n",
    "                                                          seed=i,\n",
    "                                                          n_a=36)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            tab_model.append(model)\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'pred_tab_seed{i}.npy', nn_preds[0])\n",
    "            model.save_model(f'artifacts/tabnet_fold_0_seed{i}')\n",
    "            \n",
    "    tab_model = get_top_n_models(tab_model, scores, TAB_MODEL_TOP_N)\n",
    "    print(f'total {len(tab_model)} models will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310b139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:57:18.576429Z",
     "iopub.status.busy": "2022-01-23T06:57:18.575179Z",
     "iopub.status.idle": "2022-01-23T06:57:18.578624Z",
     "shell.execute_reply": "2022-01-23T06:57:18.579071Z",
     "shell.execute_reply.started": "2022-01-15T04:57:17.81586Z"
    },
    "papermill": {
     "duration": 25.719783,
     "end_time": "2022-01-23T06:57:18.579245",
     "exception": false,
     "start_time": "2022-01-23T06:56:52.859462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab5e90",
   "metadata": {
    "papermill": {
     "duration": 25.580305,
     "end_time": "2022-01-23T06:58:09.549193",
     "exception": false,
     "start_time": "2022-01-23T06:57:43.968888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9627f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:58:59.994590Z",
     "iopub.status.busy": "2022-01-23T06:58:59.994023Z",
     "iopub.status.idle": "2022-01-23T06:58:59.998606Z",
     "shell.execute_reply": "2022-01-23T06:58:59.998183Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.009945Z"
    },
    "papermill": {
     "duration": 24.922124,
     "end_time": "2022-01-23T06:58:59.998728",
     "exception": false,
     "start_time": "2022-01-23T06:58:35.076604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = get_X(df_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b678419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:59:51.678805Z",
     "iopub.status.busy": "2022-01-23T06:59:51.677926Z",
     "iopub.status.idle": "2022-01-23T06:59:52.468053Z",
     "shell.execute_reply": "2022-01-23T06:59:52.468570Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.025123Z"
    },
    "papermill": {
     "duration": 26.521254,
     "end_time": "2022-01-23T06:59:52.468717",
     "exception": false,
     "start_time": "2022-01-23T06:59:25.947463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "df_pred['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "prediction_weights = {}\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "    gbdt_preds = booster.predict(X_test)\n",
    "    predictions['gbdt'] = gbdt_preds\n",
    "    prediction_weights['gbdt'] = 4\n",
    "\n",
    "\n",
    "if PREDICT_MLP and mlp_model:\n",
    "    try:\n",
    "        mlp_preds = predict_nn(X_test, mlp_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'mlp: {mlp_preds.shape}')\n",
    "        predictions['mlp'] = mlp_preds\n",
    "        prediction_weights['mlp'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict mlp: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_CNN and cnn_model:\n",
    "    try:\n",
    "        cnn_preds = predict_nn(X_test, cnn_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'cnn: {cnn_preds.shape}')\n",
    "        predictions['cnn'] = cnn_preds\n",
    "        prediction_weights['cnn'] = 4\n",
    "    except:\n",
    "        print(f'failed to predict cnn: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_TABNET and tab_model:\n",
    "    try:\n",
    "        tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD).flatten()\n",
    "        print(f'tab: {tab_preds.shape}')\n",
    "        predictions['tab'] = tab_preds\n",
    "        prediction_weights['tab'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict tab: {traceback.format_exc()}')\n",
    "\n",
    "        \n",
    "overall_preds = None\n",
    "overall_weight = np.sum(list(prediction_weights.values()))\n",
    "\n",
    "print(f'prediction will be made by: {list(prediction_weights.keys())}')\n",
    "\n",
    "for name, preds in predictions.items():\n",
    "    w = prediction_weights[name] / overall_weight\n",
    "    if overall_preds is None:\n",
    "        overall_preds = preds * w\n",
    "    else:\n",
    "        overall_preds += preds * w\n",
    "        \n",
    "df_pred['target'] = np.clip(overall_preds, 0, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a77be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T07:00:43.579503Z",
     "iopub.status.busy": "2022-01-23T07:00:43.578812Z",
     "iopub.status.idle": "2022-01-23T07:00:43.602164Z",
     "shell.execute_reply": "2022-01-23T07:00:43.602580Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.056985Z"
    },
    "papermill": {
     "duration": 25.584209,
     "end_time": "2022-01-23T07:00:43.602727",
     "exception": false,
     "start_time": "2022-01-23T07:00:18.018518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'sample_submission.csv'))\n",
    "submission = pd.merge(sub[['row_id']], df_pred[['row_id', 'target']], how='left')\n",
    "submission['target'] = submission['target'].fillna(0)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16517.385856,
   "end_time": "2022-01-23T07:01:10.301218",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-23T02:25:52.915362",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
