{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15101533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\samsung\\anaconda3\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: wheel in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from lightgbm) (0.24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from lightgbm) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\samsung\\anaconda3\\lib\\site-packages (12.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pyarrow) (1.22.4)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\samsung\\anaconda3\\lib\\site-packages (2023.7.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from fastparquet) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from fastparquet) (1.22.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from fastparquet) (21.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from fastparquet) (2021.10.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from packaging->fastparquet) (3.0.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\samsung\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94797b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\samsung\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: dask[complete] in c:\\users\\samsung\\anaconda3\\lib\\site-packages (2023.7.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (6.8.0)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (8.0.3)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (2021.10.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (1.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (2.0.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (21.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (4.3.2)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (12.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from click>=8.0->dask[complete]) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from packaging>=20.0->dask[complete]) (3.0.4)\n",
      "Requirement already satisfied: locket in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from partd>=1.2.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (2.11.3)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (3.2.0)\n",
      "Requirement already satisfied: distributed==2023.7.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from dask[complete]) (2023.7.0)\n",
      "Requirement already satisfied: zict>=2.2.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from distributed==2023.7.0->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from distributed==2023.7.0->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from distributed==2023.7.0->dask[complete]) (1.0.2)\n",
      "Requirement already satisfied: tornado>=6.0.4 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from distributed==2023.7.0->dask[complete]) (6.1)\n",
      "Requirement already satisfied: tblib>=1.6.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from distributed==2023.7.0->dask[complete]) (1.7.0)\n",
      "Requirement already satisfied: psutil>=5.7.2 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from distributed==2023.7.0->dask[complete]) (5.8.0)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from distributed==2023.7.0->dask[complete]) (1.26.7)\n",
      "Requirement already satisfied: contourpy>=1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from bokeh>=2.4.2->dask[complete]) (1.1.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from bokeh>=2.4.2->dask[complete]) (8.4.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from bokeh>=2.4.2->dask[complete]) (2023.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from jinja2>=2.10.3->dask[complete]) (1.1.1)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\samsung\\anaconda3\\lib\\site-packages (12.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\samsung\\anaconda3\\lib\\site-packages (from pyarrow) (1.22.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas \"dask[complete]\"\n",
    "!pip install -U pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6531bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:25:59.618292Z",
     "iopub.status.busy": "2022-01-23T02:25:59.616635Z",
     "iopub.status.idle": "2022-01-23T02:26:03.021510Z",
     "shell.execute_reply": "2022-01-23T02:26:03.020936Z",
     "shell.execute_reply.started": "2022-01-19T11:20:17.036084Z"
    },
    "papermill": {
     "duration": 3.439413,
     "end_time": "2022-01-23T02:26:03.021680",
     "exception": false,
     "start_time": "2022-01-23T02:25:59.582267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc  # 가비지 컬렉션 모듈(gc)을 가져옵니다.\n",
    "import glob  # 파일 경로를 찾기 위한 glob 모듈을 가져옵니다.\n",
    "import os  # 운영 체제와 상호 작용하기 위한 os 모듈을 가져옵니다.\n",
    "import time  # 시간 관련 기능을 사용하기 위한 time 모듈을 가져옵니다.\n",
    "import traceback  # 예외 정보를 출력하기 위한 traceback 모듈을 가져옵니다.\n",
    "from contextlib import contextmanager  # 컨텍스트 관리자를 사용하기 위한 contextlib 모듈을 가져옵니다.\n",
    "from enum import Enum  # 열거형을 사용하기 위한 enum 모듈을 가져옵니다.\n",
    "from typing import Dict, List, Optional, Tuple  # 타입 힌트를 사용하기 위한 typing 모듈을 가져옵니다.\n",
    "\n",
    "import seaborn as sns  # 시각화 라이브러리인 seaborn을 가져옵니다.\n",
    "import pandas as pd  # 데이터 조작을 위한 pandas 라이브러리를 가져옵니다.\n",
    "import numpy as np  # 수학 및 배열 연산을 위한 numpy 라이브러리를 가져옵니다.\n",
    "import matplotlib.pyplot as plt  # 시각화 라이브러리인 matplotlib를 가져옵니다.\n",
    "import lightgbm as lgb  # 경량 부스팅 머신러닝 라이브러리인 lightgbm을 가져옵니다.\n",
    "from IPython.display import display  # IPython에서 출력을 보여주기 위한 display 모듈을 가져옵니다.\n",
    "\n",
    "from joblib import delayed, Parallel  # 병렬 처리를 위한 joblib 모듈을 가져옵니다.\n",
    "from sklearn.decomposition import LatentDirichletAllocation  # LDA(Latent Dirichlet Allocation)를 사용하기 위한 모듈을 가져옵니다.\n",
    "from sklearn.manifold import TSNE  # t-SNE(t-Distributed Stochastic Neighbor Embedding)를 사용하기 위한 모듈을 가져옵니다.\n",
    "from sklearn.model_selection import GroupKFold  # 그룹 기반 교차 검증을 위한 모듈을 가져옵니다.\n",
    "from sklearn.neighbors import NearestNeighbors  # 최근접 이웃 알고리즘을 사용하기 위한 모듈을 가져옵니다.\n",
    "from sklearn.preprocessing import minmax_scale  # 데이터 스케일링을 위한 모듈을 가져옵니다.\n",
    "from tqdm import tqdm_notebook as tqdm  # 진행 상황을 표시하기 위한 tqdm 모듈을 가져옵니다.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = ''  # 데이터 디렉토리를 지정합니다.\n",
    "\n",
    "# 데이터 구성 설정\n",
    "USE_PRECOMPUTE_FEATURES = True  # train.csv의 미리 계산된 특징을 로드합니다(속도 향상을 위해 사용)\n",
    "\n",
    "# 모델 및 앙상블 설정\n",
    "PREDICT_CNN = True  # CNN 모델 예측 여부\n",
    "PREDICT_MLP = True  # MLP 모델 예측 여부\n",
    "PREDICT_GBDT = True  # GBDT 모델 예측 여부\n",
    "PREDICT_TABNET = False  # TabNet 모델 예측 여부\n",
    "\n",
    "GBDT_NUM_MODELS = 5  # GBDT 모델 개수\n",
    "GBDT_LR = 0.02  # GBDT 학습률\n",
    "\n",
    "NN_VALID_TH = 0.185  # 최근접 이웃 유효성 임계값\n",
    "NN_MODEL_TOP_N = 3  # 최근접 이웃 모델 상위 N개 선택\n",
    "TAB_MODEL_TOP_N = 3  # TabNet 모델 상위 N개 선택\n",
    "ENSEMBLE_METHOD = 'mean'  # 앙상블 방법('mean' 또는 'other')\n",
    "NN_NUM_MODELS = 10  # 최근접 이웃 모델 개수\n",
    "TABNET_NUM_MODELS = 5  # TabNet 모델 개수\n",
    "\n",
    "# GPU 할당 절약을 위한 설정\n",
    "IS_1ST_STAGE = False  # 1단계인지 여부\n",
    "SHORTCUT_NN_IN_1ST_STAGE = False  # GPU 할당 절약을 위한 조기 중단(최근접 이웃 모델)\n",
    "SHORTCUT_GBDT_IN_1ST_STAGE = False  # GPU 할당 절약을 위한 조기 중단(GBDT 모델)\n",
    "MEMORY_TEST_MODE = False  # 메모리 테스트 모드\n",
    "\n",
    "# 변수 중요도 분석을 위한 설정\n",
    "CV_SPLIT = 'time'  # 교차 검증 분할 방법('time': 시계열 KFold, 'group': 그룹 기반 KFold)\n",
    "USE_PRICE_NN_FEATURES = True  # 가격에 의존하는 최근접 이웃 특징 사용 여부\n",
    "USE_VOL_NN_FEATURES = True  # 거래량에 의존하는 최근접 이웃 특징 사용 여부\n",
    "USE_SIZE_NN_FEATURES = True  # 주문량에 의존하는 최근접 이웃 특징 사용 여부\n",
    "USE_RANDOM_NN_FEATURES = False  # 무작위 인덱스를 사용하여 이웃을 집계하는 최근접 이웃 특징 사용 여부\n",
    "\n",
    "USE_TIME_ID_NN = True  # 시간 ID를 기반으로 한 이웃 사용 여부\n",
    "USE_STOCK_ID_NN = True  # 주식 ID를 기반으로 한 이웃 사용 여부\n",
    "\n",
    "ENABLE_RANK_NORMALIZATION = True  # 순위 정규화 사용 여부\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n",
    "\n",
    "\n",
    "def print_trace(name: str = ''):\n",
    "    print(f'{name or \"익명\"}에서 에러가 발생했습니다.')\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9283ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.105533Z",
     "iopub.status.busy": "2022-01-23T02:26:30.104748Z",
     "iopub.status.idle": "2022-01-23T02:26:30.415719Z",
     "shell.execute_reply": "2022-01-23T02:26:30.416330Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.625381Z"
    },
    "papermill": {
     "duration": 0.341799,
     "end_time": "2022-01-23T02:26:30.416539",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.074740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')  # 'train.csv' 파일을 읽어와서 train 변수에 저장합니다.\n",
    "stock_ids = set(train['stock_id'])  # train 데이터의 'stock_id' 열을 추출하여 중복을 제거한 후 stock_ids 변수에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67a8c8",
   "metadata": {
    "papermill": {
     "duration": 0.030693,
     "end_time": "2022-01-23T02:26:30.479103",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.448410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9bcf32",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.573125Z",
     "iopub.status.busy": "2022-01-23T02:26:30.553308Z",
     "iopub.status.idle": "2022-01-23T02:26:30.575466Z",
     "shell.execute_reply": "2022-01-23T02:26:30.575064Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.920189Z"
    },
    "papermill": {
     "duration": 0.067985,
     "end_time": "2022-01-23T02:26:30.575573",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.507588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum  # 열거형을 사용하기 위한 enum 모듈을 가져옵니다.\n",
    "import pandas as pd  # 데이터 조작을 위한 pandas 라이브러리를 가져옵니다.\n",
    "import numpy as np  # 수학 및 배열 연산을 위한 numpy 라이브러리를 가져옵니다.\n",
    "from typing import Dict, List, Optional, Tuple  # 타입 힌트를 사용하기 위한 typing 모듈을 가져옵니다.\n",
    "from joblib import delayed, Parallel  # 병렬 처리를 위한 joblib 모듈을 가져옵니다.\n",
    "\n",
    "\n",
    "class DataBlock(Enum):  # DataBlock 열거형 클래스를 정의합니다.\n",
    "    TRAIN = 1  # 학습 데이터 블록\n",
    "    TEST = 2  # 테스트 데이터 블록\n",
    "    BOTH = 3  # 학습 및 테스트 데이터 블록\n",
    "\n",
    "\n",
    "def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n",
    "    df = pd.read_parquet('./'+directory+'/stock_id={}'.format(stock_id), engine=\"pyarrow\")  # 주어진 디렉토리에서 stock_id에 해당하는 데이터를 읽어옵니다.\n",
    "    #print(df)  # 읽어온 데이터프레임을 출력합니다.\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n",
    "    if block == DataBlock.TRAIN:\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')  # 학습 데이터를 읽어옵니다.\n",
    "    elif block == DataBlock.TEST:\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')  # 테스트 데이터를 읽어옵니다.\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, DataBlock.TRAIN),\n",
    "            load_data(stock_id, stem, DataBlock.TEST)\n",
    "        ]).reset_index(drop=True)  # 학습 및 테스트 데이터를 결합한 후 인덱스를 재설정합니다.\n",
    "\n",
    "\n",
    "def load_book(stock_id: int, block: DataBlock = DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'book', block)  # 주어진 stock_id와 데이터 블록에 해당하는 book 데이터를 읽어옵니다.\n",
    "\n",
    "\n",
    "def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'trade', block)  # 주어진 stock_id와 데이터 블록에 해당하는 trade 데이터를 읽어옵니다.\n",
    "\n",
    "\n",
    "def calc_wap1(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (\n",
    "            df['bid_size1'] + df['ask_size1'])  # wap1을 계산합니다.\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (\n",
    "            df['bid_size2'] + df['ask_size2'])  # wap2를 계산합니다.\n",
    "    return wap\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def make_book_feature(stock_id, block=DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)  # book 데이터를 가져옵니다.\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)  # wap1을 계산하여 'wap1' 열을 추가합니다.\n",
    "    book['wap2'] = calc_wap2(book)  # wap2를 계산하여 'wap2' 열을 추가합니다.\n",
    "    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return).reset_index(drop=True)  # wap1의 로그 수익률을 계산하여 'log_return1' 열을 추가합니다.\n",
    "    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return).reset_index(drop=True)  # wap2의 로그 수익률을 계산하여 'log_return2' 열을 추가합니다.\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return).reset_index(drop=True)  # ask_price1의 로그 수익률을 계산하여 'log_return_ask1' 열을 추가합니다.\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return).reset_index(drop=True)  # ask_price2의 로그 수익률을 계산하여 'log_return_ask2' 열을 추가합니다.\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return).reset_index(drop=True)  # bid_price1의 로그 수익률을 계산하여 'log_return_bid1' 열을 추가합니다.\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return).reset_index(drop=True)  # bid_price2의 로그 수익률을 계산하여 'log_return_bid2' 열을 추가합니다.\n",
    "\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])  # wap1과 wap2의 차이인 wap_balance를 계산하여 'wap_balance' 열을 추가합니다.\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / (\n",
    "            (book['ask_price1'] + book['bid_price1']) / 2)  # 가격 스프레드를 계산하여 'price_spread' 열을 추가합니다.\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']  # bid_price1과 bid_price2의 차이인 bid_spread를 계산하여 'bid_spread' 열을 추가합니다.\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']  # ask_price1과 ask_price2의 차이인 ask_spread를 계산하여 'ask_spread' 열을 추가합니다.\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (\n",
    "            book['bid_size1'] + book['bid_size2'])  # 총 거래량을 계산하여 'total_volume' 열을 추가합니다.\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (\n",
    "            book['bid_size1'] + book['bid_size2']))  # 거래량 불균형을 계산하여 'volume_imbalance' 열을 추가합니다.\n",
    "\n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread': [np.sum, np.mean, np.std],\n",
    "        'bid_spread': [np.sum, np.mean, np.std],\n",
    "        'ask_spread': [np.sum, np.mean, np.std],\n",
    "        'total_volume': [np.sum, np.mean, np.std],\n",
    "        'volume_imbalance': [np.sum, np.mean, np.std]\n",
    "    }\n",
    "\n",
    "    agg = book.groupby('time_id').agg(features).reset_index(drop=False)  # time_id로 그룹화하여 통계량을 계산합니다.\n",
    "    agg.columns = flatten_name('book', agg.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "    agg['stock_id'] = stock_id  # stock_id를 추가합니다.\n",
    "\n",
    "    for time in [450, 300, 150]:\n",
    "        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)  # 주어진 시간 이상의 데이터를 사용하여 통계량을 계산합니다.\n",
    "        d.columns = flatten_name(f'book_{time}', d.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')  # 계산된 통계량을 기존 데이터프레임에 병합합니다.\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_trade_feature(stock_id, block=DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)  # trade 데이터를 가져옵니다.\n",
    "    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return).reset_index(drop=True)  # price의 로그 수익률을 계산하여 'log_return' 열을 추가합니다.\n",
    "\n",
    "    features = {\n",
    "        'log_return': [realized_volatility],\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'size': [np.sum],\n",
    "        'order_count': [np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id').agg(features).reset_index()  # time_id로 그룹화하여 통계량을 계산합니다.\n",
    "    agg.columns = flatten_name('trade', agg.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "    agg['stock_id'] = stock_id  # stock_id를 추가합니다.\n",
    "\n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)  # 주어진 시간 이상의 데이터를 사용하여 통계량을 계산합니다.\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)  # 계산된 통계량의 열 이름을 변경합니다.\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')  # 계산된 통계량을 기존 데이터프레임에 병합합니다.\n",
    "    return agg\n",
    "\n",
    "\n",
    "def make_book_feature_v2(stock_id, block=DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)  # book 데이터를 가져옵니다.\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        try:\n",
    "            price_list = prices.loc[tid].values.flatten()\n",
    "            price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "            ticks[tid] = price_diff[0]\n",
    "        except Exception:\n",
    "            print_trace(f'tid={tid}')\n",
    "            ticks[tid] = np.nan\n",
    "\n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    return dst\n",
    "\n",
    "\n",
    "def make_features(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "        #df = make_extra_features(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c6b213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.639029Z",
     "iopub.status.busy": "2022-01-23T02:26:30.638303Z",
     "iopub.status.idle": "2022-01-23T02:26:41.422225Z",
     "shell.execute_reply": "2022-01-23T02:26:41.421651Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.961136Z"
    },
    "papermill": {
     "duration": 10.818325,
     "end_time": "2022-01-23T02:26:41.422361",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.604036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load feather]  1.341초\n",
      "is 1st stage\n",
      "[books]  3.011초\n",
      "[trades]  0.836초\n",
      "[extra features]  0.023초\n",
      "[books(v2)]  0.622초\n",
      "(428932, 216)\n",
      "(3, 216)\n"
     ]
    }
   ],
   "source": [
    "if USE_PRECOMPUTE_FEATURES:\n",
    "    with timer('load feather'):\n",
    "        df = pd.read_feather('features_v2.f')  # 저장된 캐시 데이터를 불러옵니다.\n",
    "else:\n",
    "    df = make_features(train, DataBlock.TRAIN)  # 학습 데이터를 기반으로 특징을 생성합니다.\n",
    "    # v2\n",
    "    df = make_features_v2(df, DataBlock.TRAIN)  # 추가 특징을 생성합니다.\n",
    "\n",
    "df.to_feather('features_v2.f')  # 생성된 특징을 캐시 파일로 저장합니다.\n",
    "\n",
    "test = pd.read_csv('test.csv')  # 테스트 데이터를 읽어옵니다.\n",
    "if len(test) == 3:\n",
    "    print('is 1st stage')\n",
    "    IS_1ST_STAGE = True\n",
    "\n",
    "if IS_1ST_STAGE and MEMORY_TEST_MODE:\n",
    "    print('use copy of training data as test data to immitate 2nd stage RAM usage.')\n",
    "    test_df = df.iloc[:170000].copy()\n",
    "    test_df['time_id'] += 32767\n",
    "    test_df['row_id'] = ''\n",
    "else:\n",
    "    test_df = make_features(test, DataBlock.TEST)  # 테스트 데이터를 기반으로 특징을 생성합니다.\n",
    "    test_df = make_features_v2(test_df, DataBlock.TEST)  # 추가 특징을 생성합니다.\n",
    "\n",
    "print(df.shape)  # 생성된 학습 데이터의 크기를 출력합니다.\n",
    "print(test_df.shape)  # 생성된 테스트 데이터의 크기를 출력합니다.\n",
    "df = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)  # 학습 데이터와 테스트 데이터를 결합하고 인덱스를 재설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d00a11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>book.seconds_in_bucket.count</th>\n",
       "      <th>book.wap1.sum</th>\n",
       "      <th>book.wap1.mean</th>\n",
       "      <th>book.wap1.std</th>\n",
       "      <th>book.wap2.sum</th>\n",
       "      <th>book.wap2.mean</th>\n",
       "      <th>book.wap2.std</th>\n",
       "      <th>...</th>\n",
       "      <th>trade_450.order_count.mean</th>\n",
       "      <th>trade_300.log_return.realized_volatility</th>\n",
       "      <th>trade_300.seconds_in_bucket.count</th>\n",
       "      <th>trade_300.size.sum</th>\n",
       "      <th>trade_300.order_count.mean</th>\n",
       "      <th>trade_150.log_return.realized_volatility</th>\n",
       "      <th>trade_150.seconds_in_bucket.count</th>\n",
       "      <th>trade_150.size.sum</th>\n",
       "      <th>trade_150.order_count.mean</th>\n",
       "      <th>tick_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>302.0</td>\n",
       "      <td>303.125061</td>\n",
       "      <td>1.003725</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>303.105539</td>\n",
       "      <td>1.003661</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>...</td>\n",
       "      <td>2.642857</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1587.0</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2069.0</td>\n",
       "      <td>2.433333</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.047768</td>\n",
       "      <td>1.000239</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>200.041171</td>\n",
       "      <td>1.000206</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>...</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>16.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>2.041667</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>188.0</td>\n",
       "      <td>187.913849</td>\n",
       "      <td>0.999542</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>187.939824</td>\n",
       "      <td>0.999680</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>...</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>120.0</td>\n",
       "      <td>119.859781</td>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>119.835941</td>\n",
       "      <td>0.998633</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>...</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1631.0</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>176.0</td>\n",
       "      <td>175.932865</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>175.934256</td>\n",
       "      <td>0.999626</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>...</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1219.0</td>\n",
       "      <td>4.909091</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>32763</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>399.0</td>\n",
       "      <td>399.721741</td>\n",
       "      <td>1.001809</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>399.714325</td>\n",
       "      <td>1.001790</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>...</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5150.0</td>\n",
       "      <td>2.813953</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7261.0</td>\n",
       "      <td>2.822581</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>32767</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>217.0</td>\n",
       "      <td>217.058914</td>\n",
       "      <td>1.000271</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>217.079727</td>\n",
       "      <td>1.000367</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3010.0</td>\n",
       "      <td>3.588235</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4287.0</td>\n",
       "      <td>3.034483</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428932</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.001215</td>\n",
       "      <td>1.000405</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>3.001650</td>\n",
       "      <td>1.000550</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428933</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428934</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428935 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target  book.seconds_in_bucket.count  \\\n",
       "0              0        5  0.004136                         302.0   \n",
       "1              0       11  0.001445                         200.0   \n",
       "2              0       16  0.002168                         188.0   \n",
       "3              0       31  0.002195                         120.0   \n",
       "4              0       62  0.001747                         176.0   \n",
       "...          ...      ...       ...                           ...   \n",
       "428930       126    32763  0.003357                         399.0   \n",
       "428931       126    32767  0.002090                         217.0   \n",
       "428932         0        4       NaN                           3.0   \n",
       "428933         0       32       NaN                           NaN   \n",
       "428934         0       34       NaN                           NaN   \n",
       "\n",
       "        book.wap1.sum  book.wap1.mean  book.wap1.std  book.wap2.sum  \\\n",
       "0          303.125061        1.003725       0.000693     303.105539   \n",
       "1          200.047768        1.000239       0.000262     200.041171   \n",
       "2          187.913849        0.999542       0.000864     187.939824   \n",
       "3          119.859781        0.998832       0.000757     119.835941   \n",
       "4          175.932865        0.999619       0.000258     175.934256   \n",
       "...               ...             ...            ...            ...   \n",
       "428930     399.721741        1.001809       0.000456     399.714325   \n",
       "428931     217.058914        1.000271       0.000384     217.079727   \n",
       "428932       3.001215        1.000405       0.000170       3.001650   \n",
       "428933            NaN             NaN            NaN            NaN   \n",
       "428934            NaN             NaN            NaN            NaN   \n",
       "\n",
       "        book.wap2.mean  book.wap2.std  ...  trade_450.order_count.mean  \\\n",
       "0             1.003661       0.000781  ...                    2.642857   \n",
       "1             1.000206       0.000272  ...                    2.200000   \n",
       "2             0.999680       0.000862  ...                    3.666667   \n",
       "3             0.998633       0.000656  ...                    3.666667   \n",
       "4             0.999626       0.000317  ...                    3.500000   \n",
       "...                ...            ...  ...                         ...   \n",
       "428930        1.001790       0.000507  ...                    2.727273   \n",
       "428931        1.000367       0.000465  ...                    4.000000   \n",
       "428932        1.000550       0.000153  ...                         NaN   \n",
       "428933             NaN            NaN  ...                         NaN   \n",
       "428934             NaN            NaN  ...                         NaN   \n",
       "\n",
       "        trade_300.log_return.realized_volatility  \\\n",
       "0                                       0.001308   \n",
       "1                                       0.000587   \n",
       "2                                       0.001137   \n",
       "3                                       0.001089   \n",
       "4                                       0.000453   \n",
       "...                                          ...   \n",
       "428930                                  0.001520   \n",
       "428931                                  0.000849   \n",
       "428932                                       NaN   \n",
       "428933                                       NaN   \n",
       "428934                                       NaN   \n",
       "\n",
       "        trade_300.seconds_in_bucket.count  trade_300.size.sum  \\\n",
       "0                                    21.0              1587.0   \n",
       "1                                    16.0               900.0   \n",
       "2                                    12.0              1189.0   \n",
       "3                                     9.0              1556.0   \n",
       "4                                    11.0              1219.0   \n",
       "...                                   ...                 ...   \n",
       "428930                               43.0              5150.0   \n",
       "428931                               17.0              3010.0   \n",
       "428932                                NaN                 NaN   \n",
       "428933                                NaN                 NaN   \n",
       "428934                                NaN                 NaN   \n",
       "\n",
       "        trade_300.order_count.mean  trade_150.log_return.realized_volatility  \\\n",
       "0                         2.571429                                  0.001701   \n",
       "1                         2.250000                                  0.000813   \n",
       "2                         3.166667                                  0.001621   \n",
       "3                         5.111111                                  0.001401   \n",
       "4                         4.909091                                  0.000550   \n",
       "...                            ...                                       ...   \n",
       "428930                    2.813953                                  0.001714   \n",
       "428931                    3.588235                                  0.001012   \n",
       "428932                         NaN                                       NaN   \n",
       "428933                         NaN                                       NaN   \n",
       "428934                         NaN                                       NaN   \n",
       "\n",
       "        trade_150.seconds_in_bucket.count  trade_150.size.sum  \\\n",
       "0                                    30.0              2069.0   \n",
       "1                                    24.0              1173.0   \n",
       "2                                    20.0              2010.0   \n",
       "3                                    11.0              1631.0   \n",
       "4                                    16.0              1570.0   \n",
       "...                                   ...                 ...   \n",
       "428930                               62.0              7261.0   \n",
       "428931                               29.0              4287.0   \n",
       "428932                                NaN                 NaN   \n",
       "428933                                NaN                 NaN   \n",
       "428934                                NaN                 NaN   \n",
       "\n",
       "        trade_150.order_count.mean  tick_size  \n",
       "0                         2.433333   0.000052  \n",
       "1                         2.041667   0.000050  \n",
       "2                         2.950000   0.000048  \n",
       "3                         4.545455   0.000046  \n",
       "4                         4.500000   0.000047  \n",
       "...                            ...        ...  \n",
       "428930                    2.822581   0.000066  \n",
       "428931                    3.034483   0.000051  \n",
       "428932                         NaN   0.000049  \n",
       "428933                         NaN        NaN  \n",
       "428934                         NaN        NaN  \n",
       "\n",
       "[428935 rows x 216 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281cfe2",
   "metadata": {
    "papermill": {
     "duration": 0.028421,
     "end_time": "2022-01-23T02:26:41.480303",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.451882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Nearest-Neighbor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aca7436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.555136Z",
     "iopub.status.busy": "2022-01-23T02:26:41.550462Z",
     "iopub.status.idle": "2022-01-23T02:26:41.557458Z",
     "shell.execute_reply": "2022-01-23T02:26:41.557058Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.104849Z"
    },
    "papermill": {
     "duration": 0.048663,
     "end_time": "2022-01-23T02:26:41.557564",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.508901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 80\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: Optional[Dict] = None, \n",
    "                 exclude_self: bool = False):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        \n",
    "        if metric == 'random':\n",
    "            n_queries = len(pivot)\n",
    "            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n",
    "        else:\n",
    "            nn = NearestNeighbors(\n",
    "                n_neighbors=N_NEIGHBORS_MAX, \n",
    "                p=p, \n",
    "                metric=metric, \n",
    "                metric_params=metric_params\n",
    "            )\n",
    "            nn.fit(pivot)\n",
    "            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,:], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.unstack().reset_index()\n",
    "        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n",
    "        return dst\n",
    "# index='time_id', columns='stock_id', values='price'\n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        feature_pivot = df.pivot(index='time_id', columns='stock_id', values=feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "        feature_pivot.head()\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n",
    "\n",
    "\n",
    "class StockIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"stock-id based nearest neighbor features\"\"\"\n",
    "        feature_pivot = df.pivot(index='time_id', columns='stock_id', values=feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972a076e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.619972Z",
     "iopub.status.busy": "2022-01-23T02:26:41.619012Z",
     "iopub.status.idle": "2022-01-23T02:26:41.982083Z",
     "shell.execute_reply": "2022-01-23T02:26:41.981564Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.127333Z"
    },
    "papermill": {
     "duration": 0.395558,
     "end_time": "2022-01-23T02:26:41.982243",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.586685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\n",
    "df['trade.tau'] = np.sqrt(1 / df['trade.seconds_in_bucket.count'])\n",
    "df['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\n",
    "df['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\n",
    "df['real_price'] = 0.01 / df['tick_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a6240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T02:18:50.195022Z",
     "iopub.status.busy": "2022-01-16T02:18:50.1946Z",
     "iopub.status.idle": "2022-01-16T02:18:50.201136Z",
     "shell.execute_reply": "2022-01-16T02:18:50.199965Z",
     "shell.execute_reply.started": "2022-01-16T02:18:50.194964Z"
    },
    "papermill": {
     "duration": 0.030837,
     "end_time": "2022-01-23T02:26:42.050294",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.019457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Build Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64aead97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:42.123778Z",
     "iopub.status.busy": "2022-01-23T02:26:42.122544Z",
     "iopub.status.idle": "2022-01-23T02:33:32.953387Z",
     "shell.execute_reply": "2022-01-23T02:33:32.953798Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.499414Z"
    },
    "papermill": {
     "duration": 410.874751,
     "end_time": "2022-01-23T02:33:32.953989",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.079238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[knn fit]  447.352초\n"
     ]
    }
   ],
   "source": [
    "time_id_neighbors: List[Neighbors] = []\n",
    "stock_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = df[['stock_id', 'time_id']].copy()\n",
    "    df_pv['price'] = 0.01 / df['tick_size']\n",
    "    df_pv['vol'] = df['book.log_return1.realized_volatility']\n",
    "    df_pv['trade.tau'] = df['trade.tau']\n",
    "    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n",
    "    \n",
    "    if USE_PRICE_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values='price') #  , ''\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_price_c', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='canberra', \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_price_m', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='mahalanobis',\n",
    "                metric_params={'VI':np.cov(pivot.values.T)}\n",
    "            )\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_price_l1', \n",
    "                minmax_scale(pivot.transpose()), \n",
    "                p=1, \n",
    "                exclude_self=True)\n",
    "        )\n",
    "\n",
    "    if USE_VOL_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'vol')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors('time_vol_l1', pivot, p=1)\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_vol_l1', \n",
    "                minmax_scale(pivot.transpose()), \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if USE_SIZE_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'trade.size.sum')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_size_m', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='mahalanobis', \n",
    "                metric_params={'VI':np.cov(pivot.values.T)}\n",
    "            )\n",
    "        )\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_size_c', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='canberra'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    if USE_RANDOM_NN_FEATURES:\n",
    "        pivot = df_pv.pivot(index='time_id', columns='stock_id', values= 'vol')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_random', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='random'\n",
    "            )\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_random', \n",
    "                pivot.transpose(), \n",
    "                p=2,\n",
    "                metric='random')\n",
    "        )\n",
    "\n",
    "\n",
    "if not USE_TIME_ID_NN:\n",
    "    time_id_neighbors = []\n",
    "    \n",
    "if not USE_STOCK_ID_NN:\n",
    "    stock_id_neighbors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27029360",
   "metadata": {
    "papermill": {
     "duration": 0.028479,
     "end_time": "2022-01-23T02:33:33.011086",
     "exception": false,
     "start_time": "2022-01-23T02:33:32.982607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check Neighbor Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65fd5a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.073803Z",
     "iopub.status.busy": "2022-01-23T02:33:33.073080Z",
     "iopub.status.idle": "2022-01-23T02:33:33.075920Z",
     "shell.execute_reply": "2022-01-23T02:33:33.075471Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.548287Z"
    },
    "papermill": {
     "duration": 0.035942,
     "end_time": "2022-01-23T02:33:33.076032",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.040090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_rank_correraltion(neighbors, top_n=5):\n",
    "    if not neighbors:\n",
    "        return\n",
    "    neighbor_indices = pd.DataFrame()\n",
    "    for n in neighbors:\n",
    "        neighbor_indices[n.name] = n.neighbors[:,:top_n].flatten()\n",
    "\n",
    "    sns.heatmap(neighbor_indices.corr('kendall'), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5123638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.138908Z",
     "iopub.status.busy": "2022-01-23T02:33:33.138122Z",
     "iopub.status.idle": "2022-01-23T02:33:33.192860Z",
     "shell.execute_reply": "2022-01-23T02:33:33.192422Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.555683Z"
    },
    "papermill": {
     "duration": 0.08837,
     "end_time": "2022-01-23T02:33:33.192975",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.104605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-id NN (name=time_price_c, metric=canberra, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>30183</td>\n",
       "      <td>31471</td>\n",
       "      <td>26708</td>\n",
       "      <td>7864</td>\n",
       "      <td>22752</td>\n",
       "      <td>10619</td>\n",
       "      <td>11453</td>\n",
       "      <td>1205</td>\n",
       "      <td>9352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2811</td>\n",
       "      <td>29583</td>\n",
       "      <td>30798</td>\n",
       "      <td>17639</td>\n",
       "      <td>25131</td>\n",
       "      <td>23202</td>\n",
       "      <td>14857</td>\n",
       "      <td>4739</td>\n",
       "      <td>3399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5829</td>\n",
       "      <td>4275</td>\n",
       "      <td>7783</td>\n",
       "      <td>4487</td>\n",
       "      <td>7845</td>\n",
       "      <td>25439</td>\n",
       "      <td>17530</td>\n",
       "      <td>18634</td>\n",
       "      <td>19747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>6367</td>\n",
       "      <td>19386</td>\n",
       "      <td>1255</td>\n",
       "      <td>12559</td>\n",
       "      <td>19472</td>\n",
       "      <td>18358</td>\n",
       "      <td>31719</td>\n",
       "      <td>6481</td>\n",
       "      <td>26475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>31554</td>\n",
       "      <td>24443</td>\n",
       "      <td>5916</td>\n",
       "      <td>19164</td>\n",
       "      <td>20430</td>\n",
       "      <td>659</td>\n",
       "      <td>31077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  30183  31471  26708   7864  22752  10619  11453   1205    9352\n",
       "11          11   2811  29583  30798  17639  25131  23202  14857   4739    3399\n",
       "16          16   5829   4275   7783   4487   7845  25439  17530  18634   19747\n",
       "31          31   6367  19386   1255  12559  19472  18358  31719   6481   26475\n",
       "32          32     34      4  31554  24443   5916  19164  20430    659   31077"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-id NN (name=time_price_m, metric=mahalanobis, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>31471</td>\n",
       "      <td>11453</td>\n",
       "      <td>30183</td>\n",
       "      <td>7864</td>\n",
       "      <td>26708</td>\n",
       "      <td>4091</td>\n",
       "      <td>30430</td>\n",
       "      <td>22752</td>\n",
       "      <td>9889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2811</td>\n",
       "      <td>29583</td>\n",
       "      <td>30798</td>\n",
       "      <td>14857</td>\n",
       "      <td>4739</td>\n",
       "      <td>17639</td>\n",
       "      <td>25131</td>\n",
       "      <td>23202</td>\n",
       "      <td>13745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>4275</td>\n",
       "      <td>18634</td>\n",
       "      <td>5829</td>\n",
       "      <td>25439</td>\n",
       "      <td>17530</td>\n",
       "      <td>7783</td>\n",
       "      <td>4034</td>\n",
       "      <td>4487</td>\n",
       "      <td>19747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>12559</td>\n",
       "      <td>17429</td>\n",
       "      <td>26475</td>\n",
       "      <td>31719</td>\n",
       "      <td>18358</td>\n",
       "      <td>6481</td>\n",
       "      <td>7897</td>\n",
       "      <td>12348</td>\n",
       "      <td>9456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>5916</td>\n",
       "      <td>31554</td>\n",
       "      <td>19164</td>\n",
       "      <td>6213</td>\n",
       "      <td>659</td>\n",
       "      <td>25636</td>\n",
       "      <td>24443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  31471  11453  30183   7864  26708   4091  30430  22752    9889\n",
       "11          11   2811  29583  30798  14857   4739  17639  25131  23202   13745\n",
       "16          16   4275  18634   5829  25439  17530   7783   4034   4487   19747\n",
       "31          31  12559  17429  26475  31719  18358   6481   7897  12348    9456\n",
       "32          32     34      4   5916  31554  19164   6213    659  25636   24443"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-id NN (name=time_vol_l1, metric=minkowski, p=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9352</td>\n",
       "      <td>15276</td>\n",
       "      <td>13791</td>\n",
       "      <td>1205</td>\n",
       "      <td>12923</td>\n",
       "      <td>26708</td>\n",
       "      <td>2331</td>\n",
       "      <td>2136</td>\n",
       "      <td>10672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23202</td>\n",
       "      <td>30798</td>\n",
       "      <td>17639</td>\n",
       "      <td>7460</td>\n",
       "      <td>29583</td>\n",
       "      <td>11227</td>\n",
       "      <td>2811</td>\n",
       "      <td>25131</td>\n",
       "      <td>32597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9060</td>\n",
       "      <td>25179</td>\n",
       "      <td>25439</td>\n",
       "      <td>21777</td>\n",
       "      <td>15727</td>\n",
       "      <td>17530</td>\n",
       "      <td>6476</td>\n",
       "      <td>211</td>\n",
       "      <td>30791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>10291</td>\n",
       "      <td>15689</td>\n",
       "      <td>18848</td>\n",
       "      <td>22824</td>\n",
       "      <td>14449</td>\n",
       "      <td>1142</td>\n",
       "      <td>6367</td>\n",
       "      <td>21148</td>\n",
       "      <td>25731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>25584</td>\n",
       "      <td>26883</td>\n",
       "      <td>5235</td>\n",
       "      <td>2772</td>\n",
       "      <td>26430</td>\n",
       "      <td>2502</td>\n",
       "      <td>22014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5   9352  15276  13791   1205  12923  26708   2331   2136   10672\n",
       "11          11  23202  30798  17639   7460  29583  11227   2811  25131   32597\n",
       "16          16   9060  25179  25439  21777  15727  17530   6476    211   30791\n",
       "31          31  10291  15689  18848  22824  14449   1142   6367  21148   25731\n",
       "32          34     32      4  25584  26883   5235   2772  26430   2502   22014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-id NN (name=time_size_m, metric=mahalanobis, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>30183</td>\n",
       "      <td>23490</td>\n",
       "      <td>22752</td>\n",
       "      <td>26708</td>\n",
       "      <td>20928</td>\n",
       "      <td>13791</td>\n",
       "      <td>1350</td>\n",
       "      <td>31883</td>\n",
       "      <td>10619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>29583</td>\n",
       "      <td>19417</td>\n",
       "      <td>9822</td>\n",
       "      <td>23656</td>\n",
       "      <td>4367</td>\n",
       "      <td>22828</td>\n",
       "      <td>30798</td>\n",
       "      <td>11682</td>\n",
       "      <td>10745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>25439</td>\n",
       "      <td>6121</td>\n",
       "      <td>8168</td>\n",
       "      <td>31443</td>\n",
       "      <td>7845</td>\n",
       "      <td>14721</td>\n",
       "      <td>1040</td>\n",
       "      <td>20630</td>\n",
       "      <td>11497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>13594</td>\n",
       "      <td>16802</td>\n",
       "      <td>20099</td>\n",
       "      <td>31719</td>\n",
       "      <td>1239</td>\n",
       "      <td>19472</td>\n",
       "      <td>3846</td>\n",
       "      <td>12559</td>\n",
       "      <td>13989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>22014</td>\n",
       "      <td>6482</td>\n",
       "      <td>27822</td>\n",
       "      <td>1392</td>\n",
       "      <td>9215</td>\n",
       "      <td>24921</td>\n",
       "      <td>30803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  30183  23490  22752  26708  20928  13791   1350  31883   10619\n",
       "11          11  29583  19417   9822  23656   4367  22828  30798  11682   10745\n",
       "16          16  25439   6121   8168  31443   7845  14721   1040  20630   11497\n",
       "31          31  13594  16802  20099  31719   1239  19472   3846  12559   13989\n",
       "32          32     34      4  22014   6482  27822   1392   9215  24921   30803"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time-id NN (name=time_size_c, metric=canberra, p=2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_1</th>\n",
       "      <th>top_2</th>\n",
       "      <th>top_3</th>\n",
       "      <th>top_4</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_6</th>\n",
       "      <th>top_7</th>\n",
       "      <th>top_8</th>\n",
       "      <th>top_9</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>26708</td>\n",
       "      <td>30183</td>\n",
       "      <td>22752</td>\n",
       "      <td>1205</td>\n",
       "      <td>10619</td>\n",
       "      <td>9352</td>\n",
       "      <td>15276</td>\n",
       "      <td>30620</td>\n",
       "      <td>2683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2811</td>\n",
       "      <td>17639</td>\n",
       "      <td>29583</td>\n",
       "      <td>25131</td>\n",
       "      <td>28020</td>\n",
       "      <td>17604</td>\n",
       "      <td>9822</td>\n",
       "      <td>4739</td>\n",
       "      <td>30798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>5829</td>\n",
       "      <td>4487</td>\n",
       "      <td>6121</td>\n",
       "      <td>7783</td>\n",
       "      <td>1040</td>\n",
       "      <td>29026</td>\n",
       "      <td>7845</td>\n",
       "      <td>17530</td>\n",
       "      <td>16118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>6367</td>\n",
       "      <td>12559</td>\n",
       "      <td>22519</td>\n",
       "      <td>18358</td>\n",
       "      <td>7897</td>\n",
       "      <td>19472</td>\n",
       "      <td>31522</td>\n",
       "      <td>19386</td>\n",
       "      <td>31719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>15989</td>\n",
       "      <td>11985</td>\n",
       "      <td>3732</td>\n",
       "      <td>26430</td>\n",
       "      <td>3607</td>\n",
       "      <td>10523</td>\n",
       "      <td>4487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_1  top_2  top_3  top_4  top_5  top_6  top_7  top_8  top_9  top_10\n",
       "time_id                                                                       \n",
       "5            5  26708  30183  22752   1205  10619   9352  15276  30620    2683\n",
       "11          11   2811  17639  29583  25131  28020  17604   9822   4739   30798\n",
       "16          16   5829   4487   6121   7783   1040  29026   7845  17530   16118\n",
       "31          31   6367  12559  22519  18358   7897  19472  31522  19386   31719\n",
       "32          32     34      4  15989  11985   3732  26430   3607  10523    4487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_ids = np.array(sorted(df['time_id'].unique()))\n",
    "for neighbor in time_id_neighbors:\n",
    "    print(neighbor)\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            time_ids[neighbor.neighbors[:,:10]], \n",
    "            index=pd.Index(time_ids, name='time_id'), \n",
    "            columns=[f'top_{i+1}' for i in range(10)]\n",
    "        ).iloc[1:6]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2589b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.263643Z",
     "iopub.status.busy": "2022-01-23T02:33:33.262935Z",
     "iopub.status.idle": "2022-01-23T02:33:33.276604Z",
     "shell.execute_reply": "2022-01-23T02:33:33.276173Z",
     "shell.execute_reply.started": "2022-01-19T11:39:40.610534Z"
    },
    "papermill": {
     "duration": 0.051151,
     "end_time": "2022-01-23T02:33:33.276704",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.225553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock-id NN (name=stock_price_l1, metric=minkowski, p=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top_1      64\n",
       "top_2      53\n",
       "top_3      56\n",
       "top_4     124\n",
       "top_5      73\n",
       "top_6      96\n",
       "top_7      30\n",
       "top_8      28\n",
       "top_9      66\n",
       "top_10     33\n",
       "Name: 64, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock-id NN (name=stock_vol_l1, metric=minkowski, p=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top_1      64\n",
       "top_2      20\n",
       "top_3      93\n",
       "top_4      67\n",
       "top_5      52\n",
       "top_6     107\n",
       "top_7      70\n",
       "top_8     120\n",
       "top_9     102\n",
       "top_10     39\n",
       "Name: 64, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_ids = np.array(sorted(df['stock_id'].unique()))\n",
    "for neighbor in stock_id_neighbors:\n",
    "    print(neighbor)\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            stock_ids[neighbor.neighbors[:,:10]], \n",
    "            index=pd.Index(stock_ids, name='stock_id'), \n",
    "            columns=[f'top_{i+1}' for i in range(10)]\n",
    "        ).loc[64]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e10b5471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.348457Z",
     "iopub.status.busy": "2022-01-23T02:33:33.347681Z",
     "iopub.status.idle": "2022-01-23T02:33:33.901994Z",
     "shell.execute_reply": "2022-01-23T02:33:33.902453Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.542166Z"
    },
    "papermill": {
     "duration": 0.591893,
     "end_time": "2022-01-23T02:33:33.902600",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.310707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_rank_correraltion(time_id_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35df3e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.038369Z",
     "iopub.status.busy": "2022-01-23T02:33:34.037754Z",
     "iopub.status.idle": "2022-01-23T02:33:34.275994Z",
     "shell.execute_reply": "2022-01-23T02:33:34.276373Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.949648Z"
    },
    "papermill": {
     "duration": 0.313195,
     "end_time": "2022-01-23T02:33:34.276516",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.963321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculate_rank_correraltion(stock_id_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11498c",
   "metadata": {
    "papermill": {
     "duration": 0.035477,
     "end_time": "2022-01-23T02:33:34.347529",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.312052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Aggregate Features With Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec957b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.427036Z",
     "iopub.status.busy": "2022-01-23T02:33:34.426197Z",
     "iopub.status.idle": "2022-01-23T02:33:36.057149Z",
     "shell.execute_reply": "2022-01-23T02:33:36.056625Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.189634Z"
    },
    "papermill": {
     "duration": 1.674163,
     "end_time": "2022-01-23T02:33:36.057283",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.383120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features with large changes over time are converted to relative ranks within time-id\n",
    "if ENABLE_RANK_NORMALIZATION:\n",
    "    df['trade.order_count.mean'] = df.groupby('time_id')['trade.order_count.mean'].rank()\n",
    "    df['book.total_volume.sum']  = df.groupby('time_id')['book.total_volume.sum'].rank()\n",
    "    df['book.total_volume.mean'] = df.groupby('time_id')['book.total_volume.mean'].rank()\n",
    "    df['book.total_volume.std']  = df.groupby('time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "    df['trade.tau'] = df.groupby('time_id')['trade.tau'].rank()\n",
    "\n",
    "    for dt in [150, 300, 450]:\n",
    "        df[f'book_{dt}.total_volume.sum']  = df.groupby('time_id')[f'book_{dt}.total_volume.sum'].rank()\n",
    "        df[f'book_{dt}.total_volume.mean'] = df.groupby('time_id')[f'book_{dt}.total_volume.mean'].rank()\n",
    "        df[f'book_{dt}.total_volume.std']  = df.groupby('time_id')[f'book_{dt}.total_volume.std'].rank()\n",
    "        df[f'trade_{dt}.order_count.mean'] = df.groupby('time_id')[f'trade_{dt}.order_count.mean'].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6efc7d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:36.151666Z",
     "iopub.status.busy": "2022-01-23T02:33:36.150004Z",
     "iopub.status.idle": "2022-01-23T02:33:36.152354Z",
     "shell.execute_reply": "2022-01-23T02:33:36.152748Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.199422Z"
    },
    "papermill": {
     "duration": 0.059468,
     "end_time": "2022-01-23T02:33:36.152899",
     "exception": false,
     "start_time": "2022-01-23T02:33:36.093431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    print(df2.shape)\n",
    "\n",
    "    feature_cols_stock = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade_150.tau': [np.mean],\n",
    "        'book.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "    }\n",
    "    \n",
    "    feature_cols = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'real_price': [np.max, np.mean, np.min],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "        'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n",
    "        'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "    stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            return ndf\n",
    "\n",
    "    # neighbor stock_id\n",
    "    for feature_col in feature_cols_stock.keys():\n",
    "        try:\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "\n",
    "            if not stock_id_neighbors:\n",
    "                continue\n",
    "\n",
    "            for nn in stock_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "            for agg in feature_cols_stock[feature_col]:\n",
    "                for n in stock_id_neighbor_sizes:\n",
    "                    try:\n",
    "                        for nn in stock_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('stock-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('stock-id nn')\n",
    "            pass\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "    ndf = None\n",
    "\n",
    "    print(df2.shape)\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        try:\n",
    "            if not USE_PRICE_NN_FEATURES and feature_col == 'real_price':\n",
    "                continue\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "\n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "            if 'volatility' in feature_col:\n",
    "                time_id_ns = time_id_neigbor_sizes_vol\n",
    "            else:\n",
    "                time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "    # features further derived from nearest neighbor features\n",
    "    try:\n",
    "        if USE_PRICE_NN_FEATURES:\n",
    "            for sz in time_id_neigbor_sizes:\n",
    "                denominator = f\"real_price_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
    "                df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
    "\n",
    "            for sz in time_id_neigbor_sizes_vol:\n",
    "                denominator = f\"book.log_return1.realized_volatility_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'vol_rankmin_{sz}'] = \\\n",
    "                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'vol_rankmax_{sz}'] = \\\n",
    "                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amax\"]\n",
    "\n",
    "        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "        for c in price_cols:\n",
    "            del df2[c]\n",
    "\n",
    "        if USE_PRICE_NN_FEATURES:\n",
    "            for sz in time_id_neigbor_sizes_vol:\n",
    "                tgt = f'book.log_return1.realized_volatility_nn{sz}_time_price_m_mean'\n",
    "                df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
    "    except Exception:\n",
    "        print_trace('nn features')\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d78a6c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:36.370271Z",
     "iopub.status.busy": "2022-01-23T02:33:36.368938Z",
     "iopub.status.idle": "2022-01-23T02:34:51.959576Z",
     "shell.execute_reply": "2022-01-23T02:34:51.959095Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.224929Z"
    },
    "papermill": {
     "duration": 75.77123,
     "end_time": "2022-01-23T02:34:51.959705",
     "exception": false,
     "start_time": "2022-01-23T02:33:36.188475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428935, 220)\n",
      "(428935, 280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:116: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmin_{sz}'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:118: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'vol_rankmax_{sz}'] = \\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/1622911503.py:128: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[make nearest neighbor feature]  97.638초\n",
      "(428935, 582)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df2 = make_nearest_neighbor_feature(df)\n",
    "\n",
    "print(df2.shape)\n",
    "df2.reset_index(drop=True).to_feather('optiver_df2.f')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e3ff0",
   "metadata": {
    "papermill": {
     "duration": 0.037563,
     "end_time": "2022-01-23T02:34:52.038355",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.000792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Misc Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b672b203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.076612Z",
     "iopub.status.busy": "2022-01-23T02:34:53.075859Z",
     "iopub.status.idle": "2022-01-23T02:34:53.335135Z",
     "shell.execute_reply": "2022-01-23T02:34:53.334610Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.290787Z"
    },
    "papermill": {
     "duration": 1.258742,
     "end_time": "2022-01-23T02:34:53.335258",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.076516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# skew correction for NN\n",
    "cols_to_log = [\n",
    "    'trade.size.sum',\n",
    "    'trade_150.size.sum',\n",
    "    'trade_300.size.sum',\n",
    "    'trade_450.size.sum',\n",
    "    'volume_imbalance'\n",
    "]\n",
    "for c in df2.columns:\n",
    "    for check in cols_to_log:\n",
    "        try:\n",
    "            if check in c:\n",
    "                df2[c] = np.log(df2[c]+1)\n",
    "                break\n",
    "        except Exception:\n",
    "            print_trace('log1p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76b80e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.415634Z",
     "iopub.status.busy": "2022-01-23T02:34:53.414883Z",
     "iopub.status.idle": "2022-01-23T02:34:54.757020Z",
     "shell.execute_reply": "2022-01-23T02:34:54.756480Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.724354Z"
    },
    "papermill": {
     "duration": 1.384579,
     "end_time": "2022-01-23T02:34:54.757155",
     "exception": false,
     "start_time": "2022-01-23T02:34:53.372576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/2475865050.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/2475865050.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n"
     ]
    }
   ],
   "source": [
    "# Rolling average of RV for similar trading volume\n",
    "try:\n",
    "    df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    roll_target = 'book.log_return1.realized_volatility'\n",
    "\n",
    "    for window_size in [3, 10]:\n",
    "        df2[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n",
    "            df2.groupby('stock_id')[roll_target].rolling(window_size, center=True, min_periods=1) \\\n",
    "                                                .mean() \\\n",
    "                                                .reset_index() \\\n",
    "                                                .sort_values(by=['level_1'])[roll_target].values\n",
    "except Exception:\n",
    "    print_trace('mean RV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ac17ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:54.865738Z",
     "iopub.status.busy": "2022-01-23T02:34:54.859008Z",
     "iopub.status.idle": "2022-01-23T02:34:57.630440Z",
     "shell.execute_reply": "2022-01-23T02:34:57.631709Z",
     "shell.execute_reply.started": "2022-01-15T04:54:08.318718Z"
    },
    "papermill": {
     "duration": 2.836215,
     "end_time": "2022-01-23T02:34:57.631962",
     "exception": false,
     "start_time": "2022-01-23T02:34:54.795747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/2340954359.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/2340954359.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n",
      "C:\\Users\\samsung\\AppData\\Local\\Temp/ipykernel_13384/2340954359.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n"
     ]
    }
   ],
   "source": [
    "# stock-id embedding (helps little)\n",
    "try:\n",
    "    lda_n = 3\n",
    "    lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n",
    "\n",
    "    stock_id_emb = pd.DataFrame(\n",
    "        lda.fit_transform(pivot.transpose()), \n",
    "        index=df_pv.pivot(index='time_id', columns='stock_id', values= 'vol').columns\n",
    "    )\n",
    "\n",
    "    for i in range(lda_n):\n",
    "        df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n",
    "except Exception:\n",
    "    print_trace('LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785b1249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:57.804033Z",
     "iopub.status.busy": "2022-01-23T02:34:57.802818Z",
     "iopub.status.idle": "2022-01-23T02:34:59.440689Z",
     "shell.execute_reply": "2022-01-23T02:34:59.441129Z",
     "shell.execute_reply.started": "2022-01-15T04:54:13.038956Z"
    },
    "papermill": {
     "duration": 1.736319,
     "end_time": "2022-01-23T02:34:59.441286",
     "exception": false,
     "start_time": "2022-01-23T02:34:57.704967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df2[~df2.target.isnull()].copy()\n",
    "df_test = df2[df2.target.isnull()].copy()\n",
    "del df2, df_pv\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce03106b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>book.seconds_in_bucket.count</th>\n",
       "      <th>book.wap1.sum</th>\n",
       "      <th>book.wap1.mean</th>\n",
       "      <th>book.wap1.std</th>\n",
       "      <th>book.wap2.sum</th>\n",
       "      <th>book.wap2.mean</th>\n",
       "      <th>book.wap2.std</th>\n",
       "      <th>...</th>\n",
       "      <th>book.log_return1.realized_volatility_nn3_time_price_m_mean_rank</th>\n",
       "      <th>book.log_return1.realized_volatility_nn5_time_price_m_mean_rank</th>\n",
       "      <th>book.log_return1.realized_volatility_nn10_time_price_m_mean_rank</th>\n",
       "      <th>book.log_return1.realized_volatility_nn20_time_price_m_mean_rank</th>\n",
       "      <th>book.log_return1.realized_volatility_nn40_time_price_m_mean_rank</th>\n",
       "      <th>realized_volatility_roll3_by_book.total_volume.mean</th>\n",
       "      <th>realized_volatility_roll10_by_book.total_volume.mean</th>\n",
       "      <th>stock_id_emb0</th>\n",
       "      <th>stock_id_emb1</th>\n",
       "      <th>stock_id_emb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1176</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>144.0</td>\n",
       "      <td>143.815068</td>\n",
       "      <td>0.998716</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>143.849316</td>\n",
       "      <td>0.998954</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.999327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8664</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>147.0</td>\n",
       "      <td>146.899894</td>\n",
       "      <td>0.999319</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>146.901871</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.999327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12758</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>142.0</td>\n",
       "      <td>141.728688</td>\n",
       "      <td>0.998089</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>141.709407</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.999327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>19033</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.842941</td>\n",
       "      <td>0.998329</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>93.848332</td>\n",
       "      <td>0.998387</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.999327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20499</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>170.0</td>\n",
       "      <td>169.654033</td>\n",
       "      <td>0.997965</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>169.650958</td>\n",
       "      <td>0.997947</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.999327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428930</th>\n",
       "      <td>126</td>\n",
       "      <td>16402</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>307.0</td>\n",
       "      <td>307.085632</td>\n",
       "      <td>1.000279</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>307.023346</td>\n",
       "      <td>1.000076</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.998752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428931</th>\n",
       "      <td>126</td>\n",
       "      <td>4927</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>224.0</td>\n",
       "      <td>224.815247</td>\n",
       "      <td>1.003639</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>224.783676</td>\n",
       "      <td>1.003499</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.011674</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.998752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428932</th>\n",
       "      <td>126</td>\n",
       "      <td>15155</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>348.0</td>\n",
       "      <td>337.124390</td>\n",
       "      <td>0.968748</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>336.886688</td>\n",
       "      <td>0.968065</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.014948</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.998752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428933</th>\n",
       "      <td>126</td>\n",
       "      <td>13316</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>279.0</td>\n",
       "      <td>281.655243</td>\n",
       "      <td>1.009517</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>281.599152</td>\n",
       "      <td>1.009316</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.014979</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.998752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428934</th>\n",
       "      <td>126</td>\n",
       "      <td>1464</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>506.0</td>\n",
       "      <td>503.463440</td>\n",
       "      <td>0.994987</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>503.465332</td>\n",
       "      <td>0.994991</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.009054</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.998752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428932 rows × 587 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_id  time_id    target  book.seconds_in_bucket.count  \\\n",
       "0              0     1176  0.005746                         144.0   \n",
       "1              0     8664  0.002469                         147.0   \n",
       "2              0    12758  0.002541                         142.0   \n",
       "3              0    19033  0.002515                          94.0   \n",
       "4              0    20499  0.003066                         170.0   \n",
       "...          ...      ...       ...                           ...   \n",
       "428930       126    16402  0.004163                         307.0   \n",
       "428931       126     4927  0.005008                         224.0   \n",
       "428932       126    15155  0.018900                         348.0   \n",
       "428933       126    13316  0.010262                         279.0   \n",
       "428934       126     1464  0.005431                         506.0   \n",
       "\n",
       "        book.wap1.sum  book.wap1.mean  book.wap1.std  book.wap2.sum  \\\n",
       "0          143.815068        0.998716       0.001774     143.849316   \n",
       "1          146.899894        0.999319       0.000366     146.901871   \n",
       "2          141.728688        0.998089       0.000900     141.709407   \n",
       "3           93.842941        0.998329       0.000771      93.848332   \n",
       "4          169.654033        0.997965       0.000716     169.650958   \n",
       "...               ...             ...            ...            ...   \n",
       "428930     307.085632        1.000279       0.002120     307.023346   \n",
       "428931     224.815247        1.003639       0.000781     224.783676   \n",
       "428932     337.124390        0.968748       0.006521     336.886688   \n",
       "428933     281.655243        1.009517       0.002213     281.599152   \n",
       "428934     503.463440        0.994987       0.000881     503.465332   \n",
       "\n",
       "        book.wap2.mean  book.wap2.std  ...  \\\n",
       "0             0.998954       0.001861  ...   \n",
       "1             0.999332       0.000398  ...   \n",
       "2             0.997954       0.000960  ...   \n",
       "3             0.998387       0.000798  ...   \n",
       "4             0.997947       0.000761  ...   \n",
       "...                ...            ...  ...   \n",
       "428930        1.000076       0.002213  ...   \n",
       "428931        1.003499       0.000937  ...   \n",
       "428932        0.968065       0.006172  ...   \n",
       "428933        1.009316       0.002231  ...   \n",
       "428934        0.994991       0.000936  ...   \n",
       "\n",
       "        book.log_return1.realized_volatility_nn3_time_price_m_mean_rank  \\\n",
       "0                                                    72.0                 \n",
       "1                                                    67.0                 \n",
       "2                                                    57.0                 \n",
       "3                                                    44.0                 \n",
       "4                                                    51.0                 \n",
       "...                                                   ...                 \n",
       "428930                                              101.0                 \n",
       "428931                                               77.0                 \n",
       "428932                                               80.0                 \n",
       "428933                                               90.0                 \n",
       "428934                                               98.0                 \n",
       "\n",
       "        book.log_return1.realized_volatility_nn5_time_price_m_mean_rank  \\\n",
       "0                                                    83.0                 \n",
       "1                                                    66.0                 \n",
       "2                                                    59.0                 \n",
       "3                                                    47.0                 \n",
       "4                                                    56.0                 \n",
       "...                                                   ...                 \n",
       "428930                                              105.0                 \n",
       "428931                                               76.0                 \n",
       "428932                                               82.0                 \n",
       "428933                                               90.0                 \n",
       "428934                                               97.0                 \n",
       "\n",
       "        book.log_return1.realized_volatility_nn10_time_price_m_mean_rank  \\\n",
       "0                                                    79.0                  \n",
       "1                                                    65.0                  \n",
       "2                                                    61.0                  \n",
       "3                                                    67.0                  \n",
       "4                                                    69.0                  \n",
       "...                                                   ...                  \n",
       "428930                                              105.0                  \n",
       "428931                                               84.0                  \n",
       "428932                                               82.0                  \n",
       "428933                                              102.0                  \n",
       "428934                                              101.0                  \n",
       "\n",
       "        book.log_return1.realized_volatility_nn20_time_price_m_mean_rank  \\\n",
       "0                                                    81.0                  \n",
       "1                                                    64.0                  \n",
       "2                                                    67.0                  \n",
       "3                                                    70.0                  \n",
       "4                                                    71.0                  \n",
       "...                                                   ...                  \n",
       "428930                                              105.0                  \n",
       "428931                                               85.0                  \n",
       "428932                                               79.0                  \n",
       "428933                                              102.0                  \n",
       "428934                                              102.0                  \n",
       "\n",
       "        book.log_return1.realized_volatility_nn40_time_price_m_mean_rank  \\\n",
       "0                                                    78.0                  \n",
       "1                                                    68.0                  \n",
       "2                                                    70.0                  \n",
       "3                                                    66.0                  \n",
       "4                                                    67.0                  \n",
       "...                                                   ...                  \n",
       "428930                                              105.0                  \n",
       "428931                                               90.0                  \n",
       "428932                                               87.0                  \n",
       "428933                                              103.0                  \n",
       "428934                                              103.0                  \n",
       "\n",
       "        realized_volatility_roll3_by_book.total_volume.mean  \\\n",
       "0                                                0.004140     \n",
       "1                                                0.003729     \n",
       "2                                                0.002643     \n",
       "3                                                0.002304     \n",
       "4                                                0.003089     \n",
       "...                                                   ...     \n",
       "428930                                           0.010648     \n",
       "428931                                           0.013510     \n",
       "428932                                           0.014948     \n",
       "428933                                           0.014979     \n",
       "428934                                           0.009054     \n",
       "\n",
       "        realized_volatility_roll10_by_book.total_volume.mean  stock_id_emb0  \\\n",
       "0                                                0.003038          0.000336   \n",
       "1                                                0.003409          0.000336   \n",
       "2                                                0.002964          0.000336   \n",
       "3                                                0.002818          0.000336   \n",
       "4                                                0.002721          0.000336   \n",
       "...                                                   ...               ...   \n",
       "428930                                           0.012807          0.000624   \n",
       "428931                                           0.011674          0.000624   \n",
       "428932                                           0.011329          0.000624   \n",
       "428933                                           0.011402          0.000624   \n",
       "428934                                           0.012813          0.000624   \n",
       "\n",
       "        stock_id_emb1  stock_id_emb2  \n",
       "0            0.000336       0.999327  \n",
       "1            0.000336       0.999327  \n",
       "2            0.000336       0.999327  \n",
       "3            0.000336       0.999327  \n",
       "4            0.000336       0.999327  \n",
       "...               ...            ...  \n",
       "428930       0.000624       0.998752  \n",
       "428931       0.000624       0.998752  \n",
       "428932       0.000624       0.998752  \n",
       "428933       0.000624       0.998752  \n",
       "428934       0.000624       0.998752  \n",
       "\n",
       "[428932 rows x 587 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898c5d1",
   "metadata": {
    "papermill": {
     "duration": 0.037142,
     "end_time": "2022-01-23T02:34:59.516263",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.479121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reverse Engineering time-id Order & Make CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1306bfda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.616772Z",
     "iopub.status.busy": "2022-01-23T02:34:59.615063Z",
     "iopub.status.idle": "2022-01-23T02:34:59.617414Z",
     "shell.execute_reply": "2022-01-23T02:34:59.617868Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.7715Z"
    },
    "papermill": {
     "duration": 0.063706,
     "end_time": "2022-01-23T02:34:59.618003",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.554297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "    \n",
    "\n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'], engine='pyarrow')\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        file_paths = glob.glob('book_train.parquet/**/*.parquet', recursive=True)\n",
    "\n",
    "        # stock_id 추출 및 열 생성\n",
    "        df_files = pd.DataFrame({'book_path': file_paths})\n",
    "        df_files['stock_id'] = df_files['book_path'].str.extractall(r'stock_id=(\\d+)').astype(int).reset_index(level=1, drop=True)\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot(index='time_id', columns='stock_id', values= 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    plt.plot(df_ordered['stock_id=61'])\n",
    "    \n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c3ad6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.705424Z",
     "iopub.status.busy": "2022-01-23T02:34:59.704651Z",
     "iopub.status.idle": "2022-01-23T02:35:01.635920Z",
     "shell.execute_reply": "2022-01-23T02:35:01.636972Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.801275Z"
    },
    "papermill": {
     "duration": 1.981013,
     "end_time": "2022-01-23T02:35:01.637181",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.656168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate order of time-id] 0.014sec\n",
      "folds0: train=257362, valid=42882\n",
      "folds1: train=300244, valid=42896\n",
      "folds2: train=343140, valid=42896\n",
      "folds3: train=386036, valid=42896\n",
      "[make folds] 12.012sec\n"
     ]
    }
   ],
   "source": [
    "if CV_SPLIT == 'time':\n",
    "    with timer('calculate order of time-id'):\n",
    "        if USE_PRECOMPUTE_FEATURES:\n",
    "            timeid_order = pd.read_csv('time_id_order.csv')\n",
    "        else:\n",
    "            timeid_order = reconstruct_time_id_order()\n",
    "            \n",
    "\n",
    "    with timer('make folds'):\n",
    "        timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
    "        df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
    "        df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "        folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n",
    "        time_id_orders = df_train['time_id_order']\n",
    "\n",
    "        folds = []\n",
    "        for i, border in enumerate(folds_border):\n",
    "            idx_train = np.where(time_id_orders < border)[0]\n",
    "            idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n",
    "            folds.append((idx_train, idx_valid))\n",
    "\n",
    "            print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n",
    "\n",
    "    del df_train['time_id_order']\n",
    "elif CV_SPLIT == 'group':\n",
    "    gkf = GroupKFold(n_splits=4)\n",
    "    folds = []\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(gkf.split(df_train, None, groups=df_train['time_id'])):\n",
    "        folds.append((idx_train, idx_valid))\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020eecf0",
   "metadata": {
    "papermill": {
     "duration": 0.067715,
     "end_time": "2022-01-23T02:35:01.777174",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.709459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36f6f703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:01.921148Z",
     "iopub.status.busy": "2022-01-23T02:35:01.920367Z",
     "iopub.status.idle": "2022-01-23T02:35:01.933964Z",
     "shell.execute_reply": "2022-01-23T02:35:01.934928Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.902446Z"
    },
    "papermill": {
     "duration": 0.091675,
     "end_time": "2022-01-23T02:35:01.935102",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.843427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "\n",
    "# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # order by average importance across folds\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # plot top-n\n",
    "    PLOT_TOP_N = 50\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n",
    "    return df_src[cols]\n",
    "\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "        features = list(self.models[0].feature_name())\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            assert features == list(m.feature_name())\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "        for i, m in enumerate(self.models):\n",
    "            w = self.weights[i] if self.weights is not None else 1\n",
    "            predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "        return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "    def feature_name(self) -> List[str]:\n",
    "        return self.models[0].feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb222a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:02.048565Z",
     "iopub.status.busy": "2022-01-23T02:35:02.044618Z",
     "iopub.status.idle": "2022-01-23T03:35:55.320275Z",
     "shell.execute_reply": "2022-01-23T03:35:55.319782Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.922483Z"
    },
    "papermill": {
     "duration": 3653.32245,
     "end_time": "2022-01-23T03:35:55.320410",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.997960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "lr = GBDT_LR\n",
    "if SHORTCUT_GBDT_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "    # to save GPU quota\n",
    "    lr = 0.3\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0,\n",
    "    'metric': '',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 5,\n",
    "    'min_data_in_leaf': 1000,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 128,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'learning_rate': lr\n",
    "}\n",
    "\n",
    "X = get_X(df_train)\n",
    "y = df_train['target']\n",
    "X.to_feather('X.f')\n",
    "df_train[['target']].to_feather('y.f')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "    ds = lgb.Dataset(X, y, weight=1/np.power(y, 2))\n",
    "\n",
    "    with timer('lgb.cv'):\n",
    "        ret = lgb.cv(params, ds, num_boost_round=8000, folds=folds, #cv,\n",
    "                     feval=feval_RMSPE, stratified=False, \n",
    "                     return_cvbooster=True, verbose_eval=20,\n",
    "                     early_stopping_rounds=int(40*0.1/lr))\n",
    "\n",
    "        print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n",
    "\n",
    "    best_iteration = len(ret['RMSPE-mean'])\n",
    "    for i in range(len(folds)):\n",
    "        y_pred = ret['cvbooster'].boosters[i].predict(X.iloc[folds[i][1]], num_iteration=best_iteration)\n",
    "        y_true = y.iloc[folds[i][1]]\n",
    "        print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n",
    "        \n",
    "        if i == len(folds) - 1:\n",
    "            np.save('pred_gbdt.npy', y_pred)\n",
    "\n",
    "    plot_importance(ret['cvbooster'], figsize=(10, 20))\n",
    "\n",
    "    boosters = []\n",
    "    with timer('retraining'):\n",
    "        for i in range(GBDT_NUM_MODELS):\n",
    "            params['seed'] = i\n",
    "            boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n",
    "\n",
    "    booster = EnsembleModel(boosters)\n",
    "    del ret\n",
    "    del ds\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb015e20",
   "metadata": {
    "papermill": {
     "duration": 0.057064,
     "end_time": "2022-01-23T03:35:55.434906",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.377842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce13e5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:55.569557Z",
     "iopub.status.busy": "2022-01-23T03:35:55.558558Z",
     "iopub.status.idle": "2022-01-23T03:35:56.706846Z",
     "shell.execute_reply": "2022-01-23T03:35:56.705908Z",
     "shell.execute_reply.started": "2022-01-15T04:57:16.2193Z"
    },
    "papermill": {
     "duration": 1.211778,
     "end_time": "2022-01-23T03:35:56.706992",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.495214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        scaler: Optional[StandardScaler] = None,\n",
    "        scaler_type: str = 'standard',\n",
    "        n_pca: int = -1,\n",
    "        na_cols: bool = True):\n",
    "    if na_cols:\n",
    "        #for c in X.columns:\n",
    "        for c in null_check_cols:\n",
    "            if c in X.columns:\n",
    "                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    def _pca(X_num_):\n",
    "        if n_pca > 0:\n",
    "            pca = PCA(n_components=n_pca, random_state=0)\n",
    "            return pca.fit_transform(X_num)\n",
    "        return X_num\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "\n",
    "        loss = rmspe_loss(y, model(x_num, x_cat))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=4)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def predict_tabnet(X: pd.DataFrame,\n",
    "                   model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "                   scaler: StandardScaler,\n",
    "                   ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    X_processed = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "    predicted = []\n",
    "    for m in model:\n",
    "        predicted.append(m.predict(X_processed))\n",
    "\n",
    "    if ensemble_method == 'median':\n",
    "        pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "    else:\n",
    "        pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train_tabnet(X: pd.DataFrame,\n",
    "                 y: pd.DataFrame,\n",
    "                 folds: List[Tuple],\n",
    "                 batch_size: int = 1024,\n",
    "                 lr: float = 1e-3,\n",
    "                 model_path: str = 'fold_{}.pth',\n",
    "                 scaler_type: str = 'standard',\n",
    "                 output_dir: str = 'artifacts',\n",
    "                 epochs: int = 250,\n",
    "                 seed: int = 42,\n",
    "                 n_pca: int = -1,\n",
    "                 na_cols: bool = True,\n",
    "                 patience: int = 10,\n",
    "                 factor: float = 0.5,\n",
    "                 gamma: float = 2.0,\n",
    "                 lambda_sparse: float = 8.0,\n",
    "                 n_steps: int = 2,\n",
    "                 scheduler_type: str = 'cosine',\n",
    "                 n_a: int = 16):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "        y_tr = y_tr.reshape(-1,1)\n",
    "        y_va = y_va.reshape(-1,1)\n",
    "        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "        cat_idxs = [0]\n",
    "        cat_dims = [128]\n",
    "\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "            scheduler_fn = CosineAnnealingWarmRestarts\n",
    "        else:\n",
    "            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "        model = TabNetRegressor(\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=1,\n",
    "            n_d=n_a,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            n_independent=2,\n",
    "            n_shared=2,\n",
    "            lambda_sparse=lambda_sparse,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params={'lr': lr},\n",
    "            mask_type=\"entmax\",\n",
    "            scheduler_fn=scheduler_fn,\n",
    "            scheduler_params=scheduler_params,\n",
    "            seed=seed,\n",
    "            verbose=10\n",
    "            #device_name=device,\n",
    "            #clip_value=1.5\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "        path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        model.save_model(path)\n",
    "\n",
    "        predicted = model.predict(X_va)\n",
    "\n",
    "        rmspe = rmspe_metric(y_va, predicted)\n",
    "        best_losses.append(rmspe)\n",
    "        best_predictions.append(predicted)\n",
    "\n",
    "    return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 1e10\n",
    "        best_prediction = None\n",
    "\n",
    "        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=4)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=4)\n",
    "\n",
    "        if model_type == 'mlp':\n",
    "            model = MLP(X_tr.shape[1],\n",
    "                        n_categories=[128],\n",
    "                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb, bn=mlp_bn)\n",
    "        elif model_type == 'cnn':\n",
    "            model = CNN(X_tr.shape[1],\n",
    "                        hidden_size=cnn_hidden,\n",
    "                        n_categories=[128],\n",
    "                        emb_dim=emb_dim,\n",
    "                        dropout_cat=dropout_emb,\n",
    "                        channel_1=cnn_channel1,\n",
    "                        channel_2=cnn_channel2,\n",
    "                        channel_3=cnn_channel3,\n",
    "                        two_stage=False,\n",
    "                        kernel1=cnn_kernel1,\n",
    "                        celu=cnn_celu,\n",
    "                        dropout_top=cnn_dropout,\n",
    "                        dropout_mid=cnn_dropout,\n",
    "                        dropout_bottom=cnn_dropout,\n",
    "                        weight_norm=cnn_weight_norm,\n",
    "                        leaky_relu=cnn_leaky_relu)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        model = model.to(device)\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='min',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=4)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(rmspe)\n",
    "\n",
    "            if rmspe < best_loss:\n",
    "                print(f'new best:{rmspe}')\n",
    "                best_loss = rmspe\n",
    "                best_prediction = predictions\n",
    "                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12245cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:57.029334Z",
     "iopub.status.busy": "2022-01-23T03:35:56.898214Z",
     "iopub.status.idle": "2022-01-23T06:56:27.693883Z",
     "shell.execute_reply": "2022-01-23T06:56:27.695021Z",
     "shell.execute_reply.started": "2022-01-15T04:57:17.584692Z"
    },
    "papermill": {
     "duration": 12030.930352,
     "end_time": "2022-01-23T06:56:27.695345",
     "exception": false,
     "start_time": "2022-01-23T03:35:56.764993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "del df, df_train\n",
    "gc.collect()\n",
    "\n",
    "def get_top_n_models(models, scores, top_n):\n",
    "    if len(models) <= top_n:\n",
    "        print('number of models are less than top_n. all models will be used')\n",
    "        return models\n",
    "    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n",
    "    return [x for _, x in sorted_][:top_n]\n",
    "\n",
    "\n",
    "if PREDICT_MLP:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "    \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 30\n",
    "        valid_th = NN_VALID_TH\n",
    "    \n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        # MLP\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               batch_size=512,\n",
    "                                               mlp_bn=True,\n",
    "                                               mlp_hidden=256,\n",
    "                                               mlp_dropout=0.0,\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs,\n",
    "                                               lr=0.002,\n",
    "                                               max_lr=0.0055,\n",
    "                                               weight_decay=1e-7,\n",
    "                                               model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i)\n",
    "        if nn_losses[0] < NN_VALID_TH:\n",
    "            print(f'model of seed {i} added.')\n",
    "            scores.append(nn_losses[0])\n",
    "            model_paths.append(f'artifacts/mlp_fold_0_seed{i}.pth')\n",
    "            np.save(f'pred_mlp_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    mlp_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(mlp_model)} models will be used.')\n",
    "if PREDICT_CNN:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 50\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler = train_nn(X, y, \n",
    "                                               [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               cnn_hidden=8*128,\n",
    "                                               batch_size=1280,\n",
    "                                               model_type='cnn',\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs, #epochs,\n",
    "                                               cnn_channel1=128,\n",
    "                                               cnn_channel2=3*128,\n",
    "                                               cnn_channel3=3*128,\n",
    "                                               lr=0.00038, #0.0011,\n",
    "                                               max_lr=0.0013,\n",
    "                                               weight_decay=6.5e-6,\n",
    "                                               optimizer_type='adam',\n",
    "                                               scheduler_type='reduce',\n",
    "                                               model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i,\n",
    "                                               cnn_dropout=0.0,\n",
    "                                               cnn_weight_norm=True,\n",
    "                                               cnn_leaky_relu=False,\n",
    "                                               patience=8,\n",
    "                                               factor=0.3)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            model_paths.append(f'artifacts/cnn_fold_0_seed{i}.pth')\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'pred_cnn_seed{i}.npy', nn_preds[0])\n",
    "            \n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    cnn_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(cnn_model)} models will be used.')\n",
    "    \n",
    "if PREDICT_TABNET:\n",
    "    tab_model = []\n",
    "    scores = []\n",
    "        \n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 10\n",
    "        valid_th = 1000\n",
    "    else:\n",
    "        print('train full')\n",
    "        epochs = 250\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(TABNET_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "                                                          [folds[-1]], \n",
    "                                                          batch_size=1280,\n",
    "                                                          epochs=epochs, #epochs,\n",
    "                                                          lr=0.04,\n",
    "                                                          patience=50,\n",
    "                                                          factor=0.5,\n",
    "                                                          gamma=1.6,\n",
    "                                                          lambda_sparse=3.55e-6,\n",
    "                                                          seed=i,\n",
    "                                                          n_a=36)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            tab_model.append(model)\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'pred_tab_seed{i}.npy', nn_preds[0])\n",
    "            model.save_model(f'artifacts/tabnet_fold_0_seed{i}')\n",
    "            \n",
    "    tab_model = get_top_n_models(tab_model, scores, TAB_MODEL_TOP_N)\n",
    "    print(f'total {len(tab_model)} models will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310b139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:57:18.576429Z",
     "iopub.status.busy": "2022-01-23T06:57:18.575179Z",
     "iopub.status.idle": "2022-01-23T06:57:18.578624Z",
     "shell.execute_reply": "2022-01-23T06:57:18.579071Z",
     "shell.execute_reply.started": "2022-01-15T04:57:17.81586Z"
    },
    "papermill": {
     "duration": 25.719783,
     "end_time": "2022-01-23T06:57:18.579245",
     "exception": false,
     "start_time": "2022-01-23T06:56:52.859462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab5e90",
   "metadata": {
    "papermill": {
     "duration": 25.580305,
     "end_time": "2022-01-23T06:58:09.549193",
     "exception": false,
     "start_time": "2022-01-23T06:57:43.968888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9627f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:58:59.994590Z",
     "iopub.status.busy": "2022-01-23T06:58:59.994023Z",
     "iopub.status.idle": "2022-01-23T06:58:59.998606Z",
     "shell.execute_reply": "2022-01-23T06:58:59.998183Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.009945Z"
    },
    "papermill": {
     "duration": 24.922124,
     "end_time": "2022-01-23T06:58:59.998728",
     "exception": false,
     "start_time": "2022-01-23T06:58:35.076604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = get_X(df_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b678419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:59:51.678805Z",
     "iopub.status.busy": "2022-01-23T06:59:51.677926Z",
     "iopub.status.idle": "2022-01-23T06:59:52.468053Z",
     "shell.execute_reply": "2022-01-23T06:59:52.468570Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.025123Z"
    },
    "papermill": {
     "duration": 26.521254,
     "end_time": "2022-01-23T06:59:52.468717",
     "exception": false,
     "start_time": "2022-01-23T06:59:25.947463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "df_pred['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "prediction_weights = {}\n",
    "\n",
    "if PREDICT_GBDT:\n",
    "    gbdt_preds = booster.predict(X_test)\n",
    "    predictions['gbdt'] = gbdt_preds\n",
    "    prediction_weights['gbdt'] = 4\n",
    "\n",
    "\n",
    "if PREDICT_MLP and mlp_model:\n",
    "    try:\n",
    "        mlp_preds = predict_nn(X_test, mlp_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'mlp: {mlp_preds.shape}')\n",
    "        predictions['mlp'] = mlp_preds\n",
    "        prediction_weights['mlp'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict mlp: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_CNN and cnn_model:\n",
    "    try:\n",
    "        cnn_preds = predict_nn(X_test, cnn_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'cnn: {cnn_preds.shape}')\n",
    "        predictions['cnn'] = cnn_preds\n",
    "        prediction_weights['cnn'] = 4\n",
    "    except:\n",
    "        print(f'failed to predict cnn: {traceback.format_exc()}')\n",
    "\n",
    "\n",
    "if PREDICT_TABNET and tab_model:\n",
    "    try:\n",
    "        tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD).flatten()\n",
    "        print(f'tab: {tab_preds.shape}')\n",
    "        predictions['tab'] = tab_preds\n",
    "        prediction_weights['tab'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict tab: {traceback.format_exc()}')\n",
    "\n",
    "        \n",
    "overall_preds = None\n",
    "overall_weight = np.sum(list(prediction_weights.values()))\n",
    "\n",
    "print(f'prediction will be made by: {list(prediction_weights.keys())}')\n",
    "\n",
    "for name, preds in predictions.items():\n",
    "    w = prediction_weights[name] / overall_weight\n",
    "    if overall_preds is None:\n",
    "        overall_preds = preds * w\n",
    "    else:\n",
    "        overall_preds += preds * w\n",
    "        \n",
    "df_pred['target'] = np.clip(overall_preds, 0, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a77be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T07:00:43.579503Z",
     "iopub.status.busy": "2022-01-23T07:00:43.578812Z",
     "iopub.status.idle": "2022-01-23T07:00:43.602164Z",
     "shell.execute_reply": "2022-01-23T07:00:43.602580Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.056985Z"
    },
    "papermill": {
     "duration": 25.584209,
     "end_time": "2022-01-23T07:00:43.602727",
     "exception": false,
     "start_time": "2022-01-23T07:00:18.018518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'sample_submission.csv'))\n",
    "submission = pd.merge(sub[['row_id']], df_pred[['row_id', 'target']], how='left')\n",
    "submission['target'] = submission['target'].fillna(0)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16517.385856,
   "end_time": "2022-01-23T07:01:10.301218",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-23T02:25:52.915362",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
